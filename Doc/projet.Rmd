---
title: "Projet"
output: 
  html_document:
    css: style.css
---

```{r, include = FALSE}

# Paramètres pour le .Rmd ------------------------------------------------------


knitr::opts_chunk$set(
   echo = FALSE,
   align = "c"
)


# Chemins d'accès --------------------------------------------------------------


path_data <- "../Data"
path_obj <- "../inst"
path_fct <- "../R"


# Charger les objets -----------------------------------------------------------


ind_cond <- readRDS(file.path(path_obj, "s4_ind_cond.rds"))
rf_model <- readRDS(file.path(path_obj, "s5_random_forest.rds"))
pca <- readRDS(file.path(path_obj, "s6_pca.rds"))
lasso_model <- readRDS(file.path(path_obj, "s7_lasso.rds"))
tree_model <- readRDS(file.path(path_obj, "s6_decision_tree.rds"))

river_dt <- readRDS(file.path(path_data, "s3_donnees_standardisees.rds"))


# Charger les fonctions utiles -------------------------------------------------


source(file.path(path_fct, "utils.R"))


# Librairies -------------------------------------------------------------------


libs <- c("data.table",
          "glmnet",
          "rpart.plot")

inst_load_packages(libs)


```

# Contexte et objectifs (1/2 page)

- Contexte
- Objectifs
- Hypothèses

# Préparation et nettoyage des données (1 page)

- Source
- Description
- Type de variables
- Vérifier les doublons
- Vérifier les erreurs de saisie
- Uniformisation des valeurs
- Validation de fusion de tables

# Analyse descriptive et exploratoire (2-3 pages)

- Type d’échantillon
- Nombre d'observations et variables
- Analyse descriptive uni et multivariées
- Identification des valeurs extrêmes et aberrantes (+ traitées)
- Gestion des données manquantes 
Doit se faire sur la jeu de données d'entraînement (voir p.245 ESL)
- Création de nouvelles variables
- Identification de la dépendance

# Modélisation et validation (1-2 pages)

## Pertinence du modèle

## Choix des métriques

Notre variable d'intérêt, soit le débit d'eau des rivières, est une variable continue. De ce fait, nous avons la possibilité de prendre comme métrique de performance : 

- Erreur quadratique moyen (EQM)
- Erreur absolue moyen (EAM)
- Gini <span style="color:red">Il existe il me semble le Gini pour des variables continues</span>

Nous prendrons l'erreur quatratique moyen autant pour évaluer nos modèles que comme métrique à minimiser lorsque nous optimisons les hyperparamètres pour certains modèles qui en ont.

<span style="color:red">Justifier pourquoi on ne prendre pas EAM</span>

## Stratégie de validation et choix des hyperparamètres

Notre stratégie de validation est de séparer notre jeu de données en 3 parties soit une pour l'entraînement (70%), une pour la validation des modèles et leur comparaison entre eux (15%), et une pour tester notre modèle final (15%). Pour faire la sélection d'hyperparamètres, nous faisons une validation croisée à partir  du jeu d'entraînement. Pour estimer l'erreur de généralisation basée sur ces hyperparamètres optimaux, nous allons utiliser le jeu de validation. La raison pourquoi nous n'estimons pas l'erreur de généralisation sur notre jeu d'entraînement (et pourquoi nous avons ajouté un jeu de validation) est que si nous estimons l'erreur sur le jeu d'entraînement avec des hyperparamètre qui ont été choisis pour l'optimiser, nous allons possiblement sous-estimer notre erreur de généralisation puisque nous estimons une erreur avec des hyperparamètres qui la minimise. Ainsi, pour avoir un portrait adéquat de l'erreur de généralisation, nous utilisons le jeu de validation pour faire cette estimation. 

Par ailleurs, bien que nous avons une dépendance spatiale entre les observations, nous n'allons pas user de la validation croisée par bloc pour tenir en compte cette dépendance ou encore séparer notre jeu de données en 3 de façon à faire en sorte que les observations qui ont des dépendances entre elles se trouvent dans le même jeu de données . En effet, tel que spécifier dans l'article _Cross-validation strategies for data with temporal, spatial, hierarchical, or phylogenetic structure_^[p.917, mettre la référence], lorsqu'un modèle est créé dans le but de faire des prédictions dans les mêmes territoires (donc, la même structure de dépendance) que ceux vus par le modèle en entraînement, il n'est pas recommandé d'utiliser une telle stratégie de validation, car elle peut mener à biaiser l'estimation de l'erreur de généralisation. En effet, une validation croisée par bloc ou une séparation du jeu de données considérant la dépendance aura comme effet de venir surestimer l'estimation de l'erreur de généralisation, comme elle viendra estimer l'effet de faire des prédictions sur de nouveaux territoires, ce qui n'est pas l'usage désiré de ce modèle. 

## Préselection de variables 

Comme mentionné, pour modéliser le débit d'eau des rivières, nous allons utiliser un modèle mixte. Une caractéristique importante à soulever concernant ce modèle est qu'il a la caractéristique de ne pas bien tolérer la haute dimensionnalité ainsi que la multicolinéarité (<span style="color:red">mettre une référence</span>). Ainsi, comme notre jeu de données contient plusieurs variables (~50), nous faisons, avant l'étape de modélisation, une préselection de variables. En faisant ainsi, cela nous permet d'obtenir un modèle final plus interprétable et de diminuer les risques de surapprentissage^[Gregorutti, B. & Michel, B & Saint-Pierre, P. (2017). Correlation and variable importance in random forests. Statistics and Computing, v27 n3 (201705): 659-678.]. 

Pour ce faire, nous entraînons une forêt aléatoire pour faire une sélection de variables basée sur leur importance. Par la suite, nous faisons une analyse des composantes principales pour orthogonaliser les variables explicatives et, ainsi, nous défaire de la multicolinéarité. Nous allons tenter de trouver des interactions à possiblement ajouter à notre modèle final en faisant une arbre de décision. Puis, nous entraînons un modèle avec régularisation, soit un modèle de type _Lasso_, pour finaliser cette sélection de variables.

Il est à noter que les étapes de présélection de variables qui font appel à la variable d'intérêt se font sur les jeux d'entraînement et de validation seulement. En effet, comme il est mentionné, dans _Element of Statistical Learning_ (_ELS_)^[The elements of statistical learning : data mining, inference, and prediction, p. 245], l'objectif, dans une stratégie de validation, est d'avoir un jeu de données de test qui est indépendant du modèle que l'on entraîne. Or, si nous faisons la sélection des variables explicatives, sur l'ensemble de notre jeu de données, cette indépendance ne sera pas respectée puisque le modèle sera basé sur des variables explicatives qui ont été sélectionnées partiellement à partir du jeu de test. 

Par ailleurs, nous soulignons que l'ensemble des modèles construits pour faire la présélection sont évalués en terme de performance. En effet, même si un modèle entraîné n'a pas de bonnes performances, nous avons malgré tout avoir une importance des variables ou, dans le cas du modèle _Lasso_, des coefficients contractés à 0. De ce fait, avant de faire la sélection des variables, nous allons nous assurer que le modèle construit est un modèle qui est assez performant pour l'utiliser pour faire une sélection de variable à partir de celui-ci. L'étape d'évaluation des modèles se fera sur notre jeu de données de validation.

#### Diagnostique de multicolinéarité

Avant de faire une présélection des variables à partir de la forêt aléatoire, nous venons retirer les variables qui sont fortement corrélées. En effet, l'importance des variables peut être affecté par la multicolinéarité. Dans l'article _Correlation and variable importance in random forests_^[Idem], on y souligne le risque dans un contexte de multicolinéarité d'obtenir des résultats d'importance des variables instables : une petite modification du jeu d'entraînement peut entraîner une changement complet des variables dites importantes. Dans notre jeu de données, certaines variables sont très corrélées. Nous avons, notamment, des mesures qui ont été prises à différents endroits du bassin versant qui ont pratiquement les mêmes valeurs. Nous avons, également, la situation où nous avons des prises de mesure pour différents mois subséquents qui sont quasi-parfaitement corrélés. De ce fait, nous retirons certaines variables basé sur un diagnostique de multicolinéarité. Les détails de ce diagnostique (chaque itération) se retrouve en [annexe](#multicolinearity). Ce diagnostique fait en sorte que nous retirons les variables suivantes : `r tolower(paste(translate_var(ind_cond$removed_vars, suffixe = FALSE), collapse = ", "))`.

#### Forêt aléatoire 

La forêt aléatoire entraînée pour la préselection a été construite à partir de la librairie R _ranger_ (en [annexe](#random_forest) se retrouve les détails sur la méthodologie et les performances du modèl). L'importance des variables est obtenu à partir de la permutation des variables explicatives du jeu de données _Out of the Bag_ (OOB). Comme décrit dans _ELS_^[The elements of statistical learning : data mining, inference, and prediction, p. 593], l'idée générale est que, pour chaque arbre, nous avons un jeu de donnée laissé à part (OOB) que nous utilisons pour calculer, pour chaque variable explicatives, l'augmentation de l'erreur ou la diminution de l'exactitude entre les prédictions du jeu de données non-modifié et celles du jeu de données où nous permutons les valeurs de la variable évaluée. À chaque itération, en venant calculer l'impact sur un jeu de données indépendant des données vues par l'arbre entraîné, nous avons un portrait adéquat des variables importantes en généralisation, et non, importantes pour entraîner le modèle. Dans la librairie _ranger_, l'importance des variables est calculé à partir de la méthode décrite dans l'article _A computationally fast variable importance test for random forests for high-dimensional data. Adv Data Anal Classif_^[Janitza, S., Celik, E. & Boulesteix, A.-L., (2015). A computationally fast variable importance test for random forests for high-dimensional data. Adv Data Anal Classif doi: 10.1007/s116340160276- 4]. Mentionner ça : https://stats.stackexchange.com/questions/141619/wont-highly-correlated-variables-in-random-forest-distort-accuracy-and-feature

<span style="color:red">Quelle mesure de performance pour l'importance? Comment l'interpréter? À ajouter pour trouver les limites possibles</span>

Nous affichons les `r (nb_var <- 25)` variables les plus importantes basée sur la méthode précédemment décrite :

```{r}


# Afficher les variables importantes
show_imp_var(rf_model$model, nb_var = nb_var)

# Nombre de variables conservées

rf_nb_vars <- length(which(rf_model$model$variable.importance > 5))


```

Pour sélectionner les variables, plusieurs options s'offrent à nous. Nous pouvons sélectionner soient un nombre fixé de variables explicatives. Également, nous pouvons couper à partir d'un certain seuil d'importance de variable. Nous avons décidé de garder que les variables qui ont un niveau d'importance supérieur à 5 ce qui nous a mené à conserver `r rf_nb_vars` variables.

#### Analyse des composantes principales

Nous faisons, donc, une analyse des composantes principales avec la fonction `PCA` de la librairie `FactoMineR`. Celle-ci standardise, par défaut, nos variables. <span style="color:red"> Pourquoi on a nous-même standardisée? </span> L'objectif de cette analyse des composantes principales est d'orthogonaliser nos variables pour ensuite les utiliser comme variables explicatives pour le modèle mixte. Nous faisons l'analyse des composantes principales sur l'ensemble du jeu de données comme cette procédure ne fait pas appel à la variable d'intérêt.

```{r}


eig_dt <- as.data.table(pca$eig, keep.rownames = TRUE)
names(eig_dt) <- c("comp", "eigenvalue", "pourc_var", "cum_pouc_var")
w <- which(eig_dt[order(cum_pouc_var)][["cum_pouc_var"]] > 80)


```

Nous choisissons le nombre de composantes que nous conservons. Pour ce, nous appliquons la règle du 80%, la règle de _Joliffe_ ainsi que celle de _Cattell_ :

<div style="display: flex; justify-content: center;">

<div style="padding-right: 80px;">

**Règle du 80%**

```{r}


# Afficher la variance cumulée expliquée par composante
graph_pca_var_cum(pca, threshold = 80)


```

</div>

<div style="padding-right: 80px;">

**Règle de _Joliffe_**

```{r}

apply_joliffe_rule(pca)

```

</div>

<div style="padding-right: 80px;">

**Règle de _Cattell_**

```{r}


# Afficher le graphique pour appliquer la règle de Cattell
graph_cattell(pca)


```

</div>

</div>

**Constats :**
À partir du graphique ci-dessous qui expose la règle du 80%, on voit qu'il faut pour expliquer 80% de la variabilité de nos données, conserver `r w[1]` premières composantes. Si nous utilisons la règle de _Joliffe_, qui conserve que les composantes ayant des valeurs propres > 0.7, qui est plus sévère que celle de _Kaiser_, il faut conserver les 9 premières composantes principales. Si nous utilisons plutôt la règle de Cattell, pour faire cette sélection, on observe que les valeurs propres semblent se stabiliser à partir de la composante 10. Nous pourrions, donc, garder que les 9 premières composantes. Pour être conservateurs, nous conservons, donc, les 9 premières composantes.

Nous allons afficher les trois premières composantes avec la variable de débit (qui n'a pas été inclut dans cette analyse des composantes principales). Celles-ci conservent à elles seules plus de 50% de la variabilité du jeu de données. Nous affichons que les `r (nb_var <- 10)` variables qui contribuent le plus à la variabilité de ces composantes.

<div style="display: flex; justify-content: center;">

<div style="padding-right: 80px;">

**Composantes #1 et #2**

```{r, fig.height=6, fig.width=6, fig.align="c"}


# Afficher la composante #1 et #2
graph_pca(pca, nb_var = nb_var) 


```

</div>

<div style="padding-right: 80px;">

**Composantes #2 et #3**

```{r, fig.height=6, fig.width=6, fig.align="c"}


# Afficher la composante #2 et #3
graph_pca(pca, components = c(2, 3), nb_var = nb_var)


```

</div>

</div>

#### Arbre de décision

Nous avons entraîné un arbre de décision pour sélectionner les interactions les plus importantes à tester. Pour ce faire, nous avons fixé l'arbre de décision a une profondeur de 5 pour que ça reste interprétable. Puis, nous avons optimisé le reste des hyperparamètres par validation croisée sur le jeu d'entraînement (pour plus de détails sur la méthodologie et les performances du modèle, voir la section en [annexe](#decision_tree)). Nous voyons que l'arbre dans les premiers niveaux utilise très fréquement la variable `r translate_var("riv_tc", suffixe = FALSE)` et, dans les niveaux suivants, l'`r translate_var("ria_ha", suffixe = FALSE)` ressort souvent. Nous allons tester dans le modèle _Lasso_ l'interaction entre ces deux variables. Des interactions qui arrivent qu'une fois que nous allons tester, également, sont : 

- `r translate_var("ria_ha", suffixe = FALSE)`  + `r translate_var("riv_cross_area", suffixe = FALSE)`
- `r translate_var("ria_ha", suffixe = FALSE)`  + `r translate_var("riv_larg", suffixe = FALSE)`

Pour voir l'arbre obtenu, voir l'[annexe](#decision_tree).

#### Modèle _Lasso_

Nous utilisons un modèle _Lasso_ pour compléter notre sélection de variables. Ce modèle, étant donné la contrainte sur la somme de ses coefficients, a l'avantage de venir de contracter certains coefficients à 0 et faire, par ricochet, une sélection de variables. Par ailleurs, le modèle a l'avantage de gérer les situations où nous avons deux variables fortement corrélées. En effet, dans une régression normale avec deux variables très corrélées, l'un pourrait recevoir un coefficient positif élevé et l'autre un coefficient négatif pour capturer l'effet commun aux deux variables.^[he elements of statistical learning : data mining, inference, and prediction, p. 63]. Dans un modèle _Lasso_, le coefficient de l'une de ces 2 variables sera mis à 0 éliminant l'une des variables et gérant la situation de colinéarité. En [annexe](#lasso), nous décrivons plus en détail ce modèle, notre méthodologie pour entraîner le modèle et la performance de ce dernier.

Avant de faire ce modèle, nous avons fait une modèle additif généralisé (_GAM_) pour ajouter au modèle _Lasso_ certaines transformations polynomiales. Le modèle Lasso a conservé 4 transformation polynomiales de degré 2 pour les variables  : `r tolower(paste(translate_var(lasso_model$var_poly), collapse = ", "))`. Pour plus de détails, se réfèrer à l'[annexe](#lasso). De plus, le modèle a sélectionné l'interaction : `r translate_var("riv_tc", suffixe = FALSE)` x l'`r translate_var("ria_ha", suffixe = FALSE)`.

Voici, le résulats de la validation croisée pour obtenir le $\lambda$ optimal pour minimiser l'EQM ainsi que les variables sélectionnés par le modèle avec un tel $\lambda$.

<div style="display: flex; justify-content: center;">

<div style="padding-right: 80px;">

**Validation croisée**

```{r}

lasso_model$graph_cv

```

</div>

<div style="padding-right: 80px;">

**Variables conservées**

```{r}

coefs <- coef(
   object = lasso_model$model$fit, 
   s = lasso_model$model$spec$args$penalty
)

vars <- translate_var(coefs@Dimnames[[1]][coefs@i + 1][-1])
vars <- unique(gsub(":", "", sapply(vars, function(x) substr(x, 1, nchar(x) - 4))))

lasso_nb_vars <- length(vars)

vars %>% 
   kbl(
      align = "c", 
      digits = 2,
      col.names = "Variables conservées",
      booktabs = TRUE, 
      escape = FALSE
   ) %>% 
   kable_classic(
      full_width = FALSE, 
      html_font = "Cambria"
   )

```

</div>

</div>

Nous pouvons observer qu'en dessous d'une certaine valeur, nous obtenons toujours le même $\sqrt{EQM}$. Comme nous visions à sélectionner des variables avec ce modèle, nous avons pris le $\lambda$ le plus élevé au travers ceux qui ont le plus faible EQM (voir la ligne verticale). Ainsi, nous avons sélectionné un $\lambda$ = `r lasso_model$model$spec$args$penalty`. Avec ce dernier, nous passons, donc, de `r rf_nb_vars` variables à `r lasso_nb_vars`.

# Analyse, discussion et conclusion (2-3 pages)

- Performance des modèles
- Enjeux éthiques
- Recommandation du modèle? 

# Annexe

## A.1 Diagnostique de multicolinéarité {#multicolinearity}

Voici, un portrait des itérations faits pour éliminer certaines variables qui sont hautement corrélées. À chaque étape, nous retirons la variable qui a la plus grande valeur.

- Itération 1

```{r}
table_ind_cond(ind_cond$ind_cond_1)
```

- Itération 2

```{r}
table_ind_cond(ind_cond$ind_cond_2)
```

- Itération 3

```{r}
table_ind_cond(ind_cond$ind_cond_3)
```

- Itération 4

```{r}
table_ind_cond(ind_cond$ind_cond_4)
```

Nous arrêtons les itérations, ici, car à partir de cette itération, nous n'avons plus de variable à retirer selon ce diagnostique.

## A.2 Forêt aléatoire {#random_forest}

- **Méthodologie**

La méthodologie employée pour entraîner ce modèle est composée des étapes suivantes :

1. Séparer le jeu de données en 3 parties
2. Validation croisée à 3 plis à partir du jeu d'entraînement pour optimiser les hyperparamètres (dans le but de minimiser l'EQM) :

   - Nombre de variables explicatives considérées à chaque embranchement (1, 8, 15) - Sélectionné par validation croisée : `r rf_model$model$mtry`
   - Nombre d'arbres (100, 300, 500) - Sélectionné par validation croisée : `r rf_model$model$num.trees`
   - Minimum d'observations dans un noeud (100, 550, 1000) - Sélectionné par validation croisée : `r rf_model$model$min.node.size`

3. Entraînement du modèle à partir du jeu d'entraînement avec les hyperparamètres optimaux et évaluation du modèle à partir du jeu de validation

- **Résultats**

L'EQM obtenu pour notre jeu de validation et, en comparaison, notre jeu d'entraînement :

```{r}

res <- data.table(
   rmse_train = rf_model$rmse_train$.estimate[1],
   rmse_val = rf_model$rmse_val[[1]]$.estimate[1]
)

res %>% 
   kbl(
      align = "c", 
      digits = 3,
      col.names = c("EQM (entr.)", "EQM (val.)"),
      booktabs = TRUE, 
      escape = FALSE
   ) %>% 
   kable_classic(
      full_width = FALSE, 
      html_font = "Cambria"
   )

```

**Constats :** On voit que notre modèle ne semble pas être en surapprentissage. L'EQM est de `r round(rf_model$rmse_val[[1]]$.estimate[1], 2)` sur notre jeu de données de validation. Or, cela implique qu'en moyenne l'écart de nos prédictions avec la valeur observée est  $\sqrt{EQM}$ = ~`r round(sqrt(rf_model$rmse_val[[1]]$.estimate[1]), 2)`. Pour se donner une idée de l'étendue de notre variable d'intérêt transformée :

- Moyenne : `r round(mean(river_dt$dis_m3_pyr), 3)`
- Écart-type : `r round(sd(river_dt$dis_m3_pyr), 3)`

## A.3 Arbre de décision {#decision_tree}

- **Méthodologie**

1. Séparer le jeu de données en 3 parties
2. Validation croisée à 3 plis à partir du jeu d'entraînement pour optimiser les paramètres (le paramètre de profondeur est fixé à 5) :

   - Coût de complexité (0.0000000001, 0.00000316 et 0.1) - Sélection par validation croisée : `r tree_model$model$fit$control$cp` 
   - Minimum d'observations dans un noeud (100, 550, 1000) - Sélection par validation croisée : `r tree_model$model$fit$control$minsplit`
   
3. Entraînement du modèle à partir du jeu d'entraînement avec les hyperparamètres optimaux et évaluation du modèle à partir du jeu de validation

- **Résultats**

Nous obtenons l'arbre suivant :

```{r}

rpart.plot::rpart.plot(tree_model$model$fit, roundint = FALSE)

```

L'EQM obtenu pour notre jeu de validation et, en comparaison, notre jeu d'entraînement :

```{r}

res <- data.table(
   rmse_train = tree_model$rmse_train$.estimate[1],
   rmse_val = tree_model$rmse_val[[1]]$.estimate[1]
)

res %>% 
   kbl(
      align = "c", 
      digits = 3,
      col.names = c("EQM (entr.)", "EQM (val.)"),
      booktabs = TRUE, 
      escape = FALSE
   ) %>% 
   kable_classic(
      full_width = FALSE, 
      html_font = "Cambria"
   )

```

**Constats :** On voit que notre modèle ne semble pas être en surapprentissage. L'EQM est de `r round(tree_model$rmse_val[[1]]$.estimate[1], 2)` sur notre jeu de données de validation. Or, cela implique qu'en moyenne l'écart de nos prédictions avec la valeur observée est  $\sqrt{EQM}$ = ~`r round(sqrt(tree_model$rmse_val[[1]]$.estimate[1]), 2)`. On voit sans surprise que cette arbre de décision a surappris.

## A.4 Modèle _Lasso_ {#lasso}

Un modèle _Lasso_ est une méthode de contraction des coefficients. Comparé au modèle linéaire, ce modèle se voit ajouter une contrainte, soit une valeur maximale totale qu'il peut attribuer à l'ensemble de ses coefficients. Les coefficients du modèle sont établis en minimisant l'erreur quadratique au travers les observations avec la contrainte que la somme des coefficients ne peut pas dépasser une certaine valeur :

$$\underset{\vec{\beta}}{argmin}(\sum_{i=1}^N(y_i -\hat{y}_i)^2 + \lambda\sum_{j}^p|\beta_j|)$$

Comme le démontre cette formule, plus la valeur de lambda est élevé, plus la contrainte est grande, et plus le nombre de coefficients qui risquent d'être contractés à 0 est élevé. Pour déterminer ce seul et unique hyperparamètre du modèle, nous effectuons une validation croisée sur le jeu d'entraînement. 

- **Méthodologie**

1. Séparer le jeu de données en 3 parties
2. Sélectionner à partir d'un modèle additif généralisé des transformations possibles
3. Validation croisée à 3 plis à partir du jeu d'entraînement pour optimiser le paramètre de pénalité $\lambda$ (minimiser l'EQM) : 25 valeurs entre $1 \times 10^{-10}$ et 1.
4. Entraînement du modèle à partir du jeu d'entraînement avec les hyperparamètres optimaux et évaluation du modèle à partir du jeu de validation

- **Résultats**

Avant de regarder les résultats, nous validons les hypothèses qui sous-tendent le modèle soit : l'hypothèse de linéarité, l'hypothèse d'homoscédasticité, l'hypothèse de non-corrélation et l'hypothèse de normalité des résidus. Évidemment, sachant qu'il y a de la dépendance spatiale entre certaines rivières, nous nous attendons que l'hypothèse d'homoscédasticité et de non-corrélation ne soient pas respectées.

<div style="display: flex; justify-content: center;">

<div style="padding-right: 80px;">

**Hypothèses de linéarité et d'homoscédasticité**

```{r}

lasso_model$graphs_res$res_vs_pred

```

</div>

<div style="padding-right: 80px;">

**Hypothèse de non-corrélation**

```{r}

lasso_model$graphs_res$res_vs_etiq

```

</div>

<div style="padding-right: 80px;">

**Hypothèse de normalité des résidus**

```{r}

lasso_model$graphs_res$res_qqplot

```

</div>

</div>

Voici, l'EQM obtenu pour notre jeu de validation et, en comparaison, celui avec basé sur le jeu d'entraînement :

```{r}

res <- data.table(
   rmse_train = lasso_model$rmse_train$.estimate[1],
   rmse_val = lasso_model$rmse_val[[1]]$.estimate[1]
)

res %>% 
   kbl(
      align = "c", 
      digits = 3,
      col.names = c("EQM (entr.)", "EQM (val.)"),
      booktabs = TRUE, 
      escape = FALSE
   ) %>% 
   kable_classic(
      full_width = FALSE, 
      html_font = "Cambria"
   )

```
**Constats :** L'EQM est de `r round(lasso_model$rmse_val[[1]]$.estimate[1], 2)` sur notre jeu de données de validation. Or, cela implique qu'en moyenne l'écart de nos prédictions avec la valeur observée est  $\sqrt{EQM}$ = ~`r round(sqrt(lasso_model$rmse_val[[1]]$.estimate[1]), 2)`. On remarque que l'on a un EQM plus élevé qu'avec la forêt aléatoire. Toutefois, rappelons que bien que le modèle que nous allons proposer est peut-être moins performant, le modèle est beaucoup plus interprétable.


