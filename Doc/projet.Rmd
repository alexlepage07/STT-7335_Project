---
title: "Projet"
output:
  html_document:
    css: style.css
  pdf_document: default
bibliography: projet.bib
---

```{r, include = FALSE}

# Paramètres pour le .Rmd ------------------------------------------------------


knitr::opts_chunk$set(
   echo = FALSE,
   align = "c"
)


# Chemins d'accès --------------------------------------------------------------


path_data <- "../Data"
path_obj <- "../inst"
path_fct <- "../R"


# Charger les objets -----------------------------------------------------------


ind_cond <- readRDS(file.path(path_obj, "s4_ind_cond.rds"))
rf_model <- readRDS(file.path(path_obj, "s5_random_forest.rds"))
pca <- readRDS(file.path(path_obj, "s6_pca.rds"))
lasso_model <- readRDS(file.path(path_obj, "s7_lasso.rds"))
tree_model <- readRDS(file.path(path_obj, "s6_decision_tree.rds"))

river_dt <- readRDS(file.path(path_data, "s3_donnees_standardisees.rds"))


# Charger les fonctions utiles -------------------------------------------------


source(file.path(path_fct, "utils.R"))


# Librairies -------------------------------------------------------------------


libs <- c("data.table",
          "glmnet",
          "rpart.plot")

inst_load_packages(libs)


```

## Contexte et objectifs

Selon Ressource naturelle Canada, les inondations représentent le type de catastrophes naturelles le plus coûteux au Canada^[[Ressource Naturelle Canada](https://natural-resources.canada.ca/science-and-data/science-and-research/natural-hazards/flood-mapping/24223)]. Afin de faire face à ce type de risque il va de soit de comprendre le phénomène et d'être en mesure d'identifier les zones à risque en produisant des cartes d'inondation. Pour les secteurs ayant des stations hydrologiques, la tâche est relativement simple. Cependant, lorsque l'on veut extrapoler sur des bassins versants non jaugés, la tâche se complexifie.

À ce sujet, [Chaduri et al., 2021](@chaudhuri2021inundated) a tenté une approche qui utilise un modèle linéaire qui ne se sert que de l'aire du bassin versant pour prédire la médiane du débit maximum annuel. Par la suite, il utilise une fonction log-spline pour estimer la queue de distribution à partir de la médiane prédite, et ainsi, il estime différents quantiles. Finalement, à partir de ces estimations, il utilise de l'imagerie satellitaire pour identifier les caractéristiques des rivières et, en inversant l'équation de [Manning R., 1981](@manning1891flow), on peut convertir les débits d'eau en hauteur d'eau.

Le présent projet fait suite à [Chaduri et al., 2021](@chaudhuri2021inundated) pour tenter de voir s'il est possible et utile d'avoir recours à un modèle plus élaboré pour estimer le débit d'eau pour des stations non jaugées. Notre objectif consiste donc à utiliser les données de RiverAtlas, provenant du site de [HydroSheds](https://www.hydrosheds.org/hydroatlas), pour tenter de voir quelles variables explicatives pourraient potentiellement être utilisées.

Comme les rivières sont inter-connectées les unes aux autres, une forte dépendance lie le débit de chacune d'entre elles. Pour tenir compte de cette structure de dépendance, nous proposons un modèle linéaire mixte.
Par ailleurs, comme ce projet est purement exploratoire et vise uniquement à tester le pouvoir prédictif d'un éventuel modèle de débit, nous ne cherchons pas à extrapoler sur de nouveaux bassins versants ou pour une période de temps ultérieure. En revanche, calculer un intervalle de confiance sur les débits prédits permettra d'avoir une appréciation substantielle de la performance du modèle puisque nous pourrons comparer ceux-ci aux débits moyens historiques fournis dans le jeu de données. Cette analyse viendra compléter la métrique de performance commune qu'est l'erreur quadratique moyenne (EQM).
À noter que, bien que [Chaduri et al., 2021](@chaudhuri2021inundated) fait ses prédiction sur la médiane, nous nous concentrerons sur la moyenne des débits annuels. La raison étant que les modèles mixtes standards sont fait pour estimer la moyenne d'une distribution normales et que les données de RiverAtlas ne fournissent que cette information et non pas les quantiles ou toutes les observations historiques.


## Préparation et nettoyage des données (1 page)

- Source
- Description
- Type de variables
- Vérifier les doublons
- Vérifier les erreurs de saisie
- Uniformisation des valeurs
- Validation de fusion de tables


Nous utilisons les données de l'HydroAtlas^[Pour en apprendre d'avantage sur les données utilisées, consultez le site internet d'HydoSheds à l'adresse suivante: https://data.hydrosheds.org/file/technical-documentation/HydroATLAS_TechDoc_v10_1.pdf], provenant du site de [HydroSheds](https://www.hydrosheds.org/hydroatlas) pour tenter de prédire le débit moyen des rivières pour un bassin versant donné. 
Nous focalisons nos efforts dans la province du Québec en prenant les données nord-américaine et en rognant les fichiers de formes (shapefiles) autour de la province. Le jeu de données ainsi créé contient 162 507 observations et 295 variables disponibles. Chaque observations du jeu de données correspond à un bassin versant et les variables représentent des données utiles pour effectuer des études hydrologiques, telles que les précipitations, le volume d'eau en amont, la pente hydrique, etc. La variable d'intérêt est la suivante: `dis_m3_pyr`. Celle-ci représente la moyenne annuelle des débits d'eau en 
. Le débit minimal observé est de 0.075 et il peut atteindre des valeurs aussi élevées que 12 343, avec une moyenne de 25. Ces statistiques témoignent de la forte asymétrie existant dans la distribution probabiliste du débit d'eau.

Les variables explicatives d'intérêt ont été sélectionnées manuellement en s'inspirant des variables nécessaires pour calculer l'équation de [Manning R., 1981](@manning1891flow). D'autres variables de même nature ont également été sélectionnées. Ainsi, 66 variables ont été retenues. Par la suite, les variables offrant peu ou pas d'information (variance nulle) ont été retirées, nous laissant avec 56 variables^[Les variables constituant les données sont décrites à cette adresse: https://data.hydrosheds.org/file/technical-documentation/RiverATLAS_Catalog_v10.pdf].

Parmi celles-ci, il y en a 4 qui sont catégorielles: classification du terrain, du milieu humide, des types de roche et la classification hydrologique du bassin versant.
Puis, il y a 41 variables numériques correspondant à des pourcentages.
Finalement, les 11 variables restantes sont numériques. Certaines d'entre elles, comme l'aire du bassin ou le volume hydrique, contiennent des valeurs extrêmes.

Afin de permettre l'utilisation d'un modèles linéaire mixte, il faut que la distribution sous-jacente de la variable endogène, et idéalement des variables exogènes aussi, soit normale. En outre, nous utilisons une analyse en composante principales (ACP) afin de réduire la dimension du jeu de données puisque les modèles mixtes gèrent mal les données à forte dimensionnalité. Ce faisant, cela renforce le besoin de transformer les données pour les normaliser. Pour y arriver, nous avons procédé à une transformation Box-Cox (voir [Sakira R, 1992](@boxcox1992)) de la variable endogène, ainsi que de toutes les variables numériques asymétriques.

A prori, certaines variables comportaient des valeurs qui semblaient aberrantes: des pourcentages qui comportaient des milliers et des valeurs -9999. Cependant, en lisant la documentation de celles-ci, nous avons réalisé que ces valeurs avaient leur signification qui leur était propre. Entre autre, les -9999 signifient que la donnée est manquante. 
Pour ce qui est de la variable `dor_pc_pva`, celle-ci représente la capacité d'un barrage à soutenir le débit d'une rivière. Lorsque la valeur est supérieure à 1000, cela signifie que le barrage peut soutenir le débit de plus d'une rivières à la fois. La signification étant autre, nous avons ajouté une variable indicatrice capturant cette information supplémentaire, puis nous avons plafonné cette variable à 100 par souci de cohérance avec les autres variables de même nature.
En ce qui attrait à la variable `lka_pc_cse`, celle-ci réfère au pourcentage du bassin versant occupé par des lacs. La raison pour laquelle cette variable atteignait le millier est qu'un entier occupe moins d'espace de stockage qu'un nombre réel. Or, par souci de précision, les chercheurs ayant travaillé sur cette variables ont choisit de conserver trois décimales. Ce faisant 0.001 est noté 1 et 0.999 est noté 999. Ainsi, nous avons laissé cette variable telle quelle puisque, de toute façon, lors de l'étape de l'ACP, nous normalisons les variables.


## Analyse descriptive et exploratoire (2-3 pages)

- Type d’échantillon
- Nombre d'observations et variables
- Analyse descriptive uni et multivariées
- Identification des valeurs extrêmes et aberrantes (+ traitées)
- Gestion des données manquantes 
Doit se faire sur la jeu de données d'entraînement (voir p.245 ESL)
- Création de nouvelles variables
- Identification de la dépendance

### Analyse de l'échantillon

La base de données est constituée de plus de 162000 observations pour 60 variables. Le données ont été collectées auprès de plusieurs sources différentes comprenant des universités (Berkeley par exemple), des institutions publiques ou encore de projet de collecte de données comme HydroSHEDS. Ces données n’ont donc pas été observées dans un but particulier mais surtout pour permettre leur exploitation, peu importe l’objectif et toute l’information disponible a été conservée ou résumée. Bien évidemment, la méthode de collecte de données est différente d’un sondage. On peut donc en conclure que la collecte de données est observationnelle. 

Pour chaque variable, les données ont été réanalysées de sorte à minimiser les potentielles erreurs de mesure. Le nombre de données manquantes a été minimiser également par interpolation spatiale, qui permet d’attribuer des valeurs auxquelles on n’a pas accès. On peut noter que cette interpolation est une première source de biais pour les variables qui ne sont pas fortement corrélées dans l’espace et une première surestimation de la corrélation entre les variables. En effet, certaines variables ne sont pas dépendantes dans l’espace et cette interpolation pourrait mener à une surestimation de cette dépendance. 

Nous allons donc analyser les différentes sources de biais et d’inexactitudes dans les données. Pour commencer, l’étendue des années de collecte des observations est très large, en effet, certaines observations datent du début des années 2000 et d’autres de la fin des années 2010. Dans notre jeu de données nous nous intéressons surtout aux moyennes, alors l’influence de cet aspect sur nos observations reste faible. De plus, La réanalyse effectuée devrait en théorie pouvoir corriger les erreurs et réduire les risques liées à l’écart d’âge des observations. 

 De plus, des variables sont arbitrairement bornées par les collecteurs. La variable Stream Gradient est limitée entre 60°N et 83°S par exemple. Les valeurs extrêmes sont donc par conséquent retirées du jeu de données. Cela implique directement une sous-estimation de la variance. Le nombre de données manquantes est également augmenté par ce procédé, ce qui induira possiblement un nouveau biais, selon le traitement de ces valeurs manquantes. 
 
 On remarque également une réduction des valeurs pouvant être prises par des variables pour faciliter l’utilisation des données, on peut par exemple le voir sur les variables Climate Strata et Climate Zones. En effet, les observations initiales découpaient les zones et les couches climatiques en 125 catégories et par soucis de facilité, ces catégories ont été elles-mêmes regroupées en 18 catégories plus larges. On risque alors de mettre dans la même catégorie deux zones de climat relativement différentes. 
 
 Enfin, une bonne partie des variables résulte de spéculations ou de projections dans le futur suivant des modèles. Les modèles d’évolution construits ont en général des hypothèses restrictives et possiblement des hyperparamètres à définir. En accord avec le compromis biais-variance, la complexité de ces modèles engendre une variance relativement grande entrainant alors un surapprentissage. Les valeurs prédites et l’erreur de généralisation en seront donc affectées. Mais encore, il peut exister différents modèles d’évolution pour un même phénomène, menant donc à faire un choix arbitraire. Comme exemple on peut citer la variable Potential Evapotranspiration, où l’auteur précise que le choix du modèle a été fait en fonction des résultats obtenus pour l’Afrique et l’Amérique du Sud. Le modèle peut donc être moins fiable pour les autres continents. Le même phénomène s’applique pour la variable de la végétation potentielle sans activité humaine dont le modèle a été principalement testée dans les localisations où l’activité humaine est très faible ou encore la progression des glaciers ou des permafrost dont le modèle est paramétré en fonction d’estimations de glaciers et permafrost sur la période 1961-1990. 
 
On peut également noter que l’année 2000 est prise en référence pour plusieurs variables sans explications pertinentes. On suppose que la raison principale réside dans le fait que les premières mesures « précises » ont été prises autour de cette année ou que l’année 2000 marque le début du siècle. L’arrivée de nouvelles technologies dans la collecte de données, notamment des satellites, ont révolutionné la collecte de données, ce qui justifie, au moins en partie, que l’on considère l’année 2000 comme référence. 

### Analyse et gestion des données manquantes

#### Présentation des données manquantes

Les données manquantes dans un jeu de données sont chose commune et peuvent être issues de diverses causes comme des capteurs défectueux, d'oublis lors de la collecte des données ou, encore, de mesures ajoutées en cours de route d'un projet. Notre jeu de données ne fait pas exception : nous avons des données manquantes dénotées par des _-999_'s. Pour faire le portrait des données manquantes de notre jeu de données, le Tableau 2.1.1 (Annexe A1) expose en détail les variables pour lesquelles nous avons des données manquantes et le nombre/pourcentage d'observations qui sont touchées. La variable la plus touchée est la classification des zones humides, qui contient 44% de données manquantes. Les autres variables touchées contiennent moins de 1% de données manquantes. 

#### Premiers traitements

L’acronyme *cav* désigne le bassin versant observé (*Catchment*), tandis que *uav* désigne une aggrégation des bassins en amont (*upstream*).
Nous remarquons qu'une variable ressort du lot avec un pourcentage de près de 44% de valeurs manquantes soit celle de la classification des zones humides. Or, dans la description des variables de notre jeu de données, [_RiverATLAS Attributes_](https://data.hydrosheds.org/file/technical-documentation/RiverATLAS_Catalog_v10.pdf), on y spécifie que les _-999_'s sont, en fait, les endroits n'ayant pas de zone humide ce qui explique pourquoi elles ne sont pas classifiées. Cette variable contient 12 classes numérotées de 1 à 12. Pour traiter les non-réponses de cette variable, nous y ajoutons un 13e classe qui équivaut à _aucune zone humide_. Nous remplaçons, donc, les valeurs _-999_ par un facteur correspondant à _0_.

```{r}


# Modifier les -999 par une classe pour la variable wet_cl_cmj

mod_missing_wet_cl_cmj(river_dt)


```

:Pour ce qui est des autres variables présentées dans le tableau 2.1.1 (Annexe A1), celles-ci sont bien des valeurs manquantes qui nécessitent d'être analysées et traitées.

D'une part, notons que chaque variable est mesurée pour le bassin observé ( _cav_ ) et pour les bassins en amont ( _uav_ ). Or, dans ces cas, le patron de non-réponse est très similaire. En effet, si une variable est manquante pour tous les bassins en amont, elle le sera assurément pour le bassin présent. Par ailleurs, notamment grâce au Tableau 2.1.2 (Annexe A1), il est intéressant d'observer que la corrélation entre les variables _cav_ et _uav_ sont linéairement très corrélées.

Étant donné cette forte corrélation linéaire, et considérant qu'il y a nettement moins de valeurs manquantes pour les variables portant le suffixe  _uav_ que _cav_, nous retirons ces dernières.

#### Pourcentage par patrons de non-réponse

Nous voyons dans le Tableau 2.1.3 (Annexe A1) le seul patron de non-réponse de notre jeu de données ainsi que la proportion des observations qui y est associé. Celui-ci est composé des variables de la composition du sol et il représente 0.06% de nos données. À noter que chacune de ces quatres variables proviennent d'une même source de données: [SoilGrids1km; Hengl et al. 2014](https://www.isric.org/explore/soilgrids).

Dans la documentation du jeu de données, il est précisé que les valeurs manquantes pour ces variables sont causées par la présence de grandes surfaces d'eau telles que des lacs. Pour s'en convaincre la carte interactive disponible en annexe A1 présente la localisation des bassins versants où on observe des valeurs manquantes.

#### Identification du mécanisme de non-réponse

Nous faisons une analyse préliminaire pour évaluer si les données sont manquantes de façon complètement aléatoire. Pour ce faire, nous commençons à comparer les graphique de la distribution de variables explicatives pour les observations qui ne font pas parties du patron (`patron = 0`) au graphique de la même variable mais avec les observation qui font partie de ce patron (`patron = 1`). Nous montrons en annexe 2 quelques exemples de variables dont la distribution changent en fonction de si les observations font parties du patron ou non (débit d'eau moyen, proportion occupée par des forêts...) ce qui suggère que nous ne sommes pas en présence de valeurs manquantes complètement aléatoire. 

Cette analyse préliminaire des distributions suggère que nous ne sommes pas en présence de données manquantes de façon complètement aléatoire puisque visuellement les distributions de variables pour les données observées et les données manquantes sont différentes.

Pour poursuivre notre analyse, nous exposons dans le tableau 2.1.4 (Annexe A1) l'ensemble des _test-t_'s possibles pour comparer la moyenne des autres variables pour les observations faisant parties du patron et celle pour les observations ne faisant pas parties du patron.

Nous remarquons que `r sum(t_test_res$p_value < 0.05)` tests ont un seuil observé en dessous de 5%. Cela signifie que pour ces tests la moyenne de la variable évaluée est significativement différente en présence ou non du patron de non-réponse.

En revanche, faire autant de _test-t_'s a ses limites. En effet, puisque nous faisons au total `r nrow(t_test_res)`, nous avons le problème de comparaisons multiples qui implique que, même si toutes les moyennes étaient réellement égales, une partie des tests seraient rejetée. Or, dans notre cas, une très grande partie le sont ce qui appuie notre hypothèse comme quoi les données ne sont pas manquantes complètement aléatoirement.

Pour valider nos analyses préliminaires, nous utilisons le test de _Little_, qui calcule une statistique telle que :
$$d^2 = \sum_{j = 1}^J (\hat{\mu}_j - \hat{\mu}_j^{ML})\hat\Sigma_j^{-1}(\hat{\mu}_j - \hat{\mu}_j^{ML})$$
Si les données sont manquantes de façon complètement aléatoire, nous aurons que $$d^2 \sim \chi_{\sum_{j=1}^J k_j-k}.$$ 

En utilisant la formule `naniar::mcar_test`, nous obtenons que le seuil observé est de `r res$p.value` qui confirme notre hypothèse. On rejette, donc, l'hypothèse $H_0 :$ Données manquantes avec un mécanisme _MCAR_.

#### Conclusion de l'analyse
En conclusion, comme la non-réponse pouvait être prédite selon la variable de
pourcentage de couverture des lacs dans le bassin (`lka_pc_cse`) et que
ce patron était associé à des valeurs significativement différentes pour les
autres variables, on peut affirmer que le patron de non-réponse est MAR.

Évidemment, le fait de retirer des données MAR peut introduire un biais.
Néanmoins, l'objectif de ce travail est de prédire le débit d'eau des rivières. Comme il n'y a pas de sens à calculer un débit d'eau dans un lac, nous allons simplement retirer ces observations du jeu de données.

### Réduction de la dimensionalité par ACP

#### mise en oeuvre et résultat

Notre objectif avec cette analyse en composantes principales était de faire un prétraitement des données. Nous avons été en mesure d’établir certaines relations entre différentes variables. On présente ces relations dans les lignes ci-dessous. 

Pour la première composante principale
Les variables corrélées positivement sont `pre_nn_xxx` (poids de 0,38 qui représente la quantité de précipitation), `gwt_cn_cav` (0,18 est la profondeur de la nappe phréatique) alors que `slp_dg_cav`/`uav` (0,14 correspond à la pente du terrain).

Les variables corrélées négativement sont `hdi_ix_cav` (-0,13 qui est l’indice de développement humain), `prm_pc_cse` / use (-0,11 correspond au pergélisol).

Pour la seconde composante principale
Les variables qui présentent une corrélation positive sont `rev_nc_usu` (0,34 le volume du réservoir), `ria_ha_csu`/`usu` (0,40 la surface de la rivière) et `riv_tc_csu`/`usu` (0,45 le volume de la rivière).

Les variables pour lesquelles on observe une corrélation négative correspondent à `ero_kh_cav` (-0,14 qui est l’érosion du sol) alors que `slp_dg_cav`/`uav` (-0,14 est la pente du terrain), `syr_dk_rav` (-0,12 le gradient de l’affluent) et finalement `gwt_cn_cav` (-0,12 la profondeur de la nappe phréatique).

Pour la troisième composante principale
Les variables avec une corrélation positive sont `gwt_cn_cav` (0,36 profondeur de la nappe phréatique), `slp_dg_uav`/`cav` (0,41 est la pente du terrain) tandis que `sgr_dk_rad` (0,26 est le gradient de l’affluent) et finalement `ero_hk_cav`/`uav` (0,40 correspond à l’érosion).

#### L’interprétation 

La première composante principale correspond à des rivières qui ont accès à de l’eau *courante* puisqu’on récupère de l’eau de pluie, mais la nappe phréatique serait assez profonde et on est devant un terrain qui est en pente. Possiblement que nous pourrions être en présence de rivières moins impactées par l'activité anthropique.

Quant à la deuxième composante principale, celle-ci indiquerait plus des rivières qui ont accès à des grandes quantités d’eau et l’eau serait plus statique. On est devant une corrélation négative entre la pente du terrain et l'érosion.

Pour la troisième composante principale, on serait en présence de rivières qui ont un terrain en pente ainsi qu’une nappe phréatique profonde.

Contrairement à la première composante principale, on n’observe pas de relation significative par rapport à des variables qui indiquent le volume de la rivière ou la quantité de précipitations. On remarque que la variable `dis_n3_pyr` est corrélée positivement de manière importante avec la seconde composante principale. 

On constate que 78,9% de la variabilité de la variable dis_n3_pyr est expliquée par la seconde composante principale alors que c’est 0% pour la première composante principale et 8,8% pour la troisième composante principale. Néanmoins, il est plausible de penser que la première composante principale est corrélée avec la variable d’intérêt à cause de la quantité de pluie. Il est probable que la première composante principale soit en corrélation avec la variable d’intérêt mais de façon non linéaire.

## Modélisation et validation (1-2 pages)

### Pertinence du modèle

Le modèle proposé pour notre analyse est un modèle mixte. En effet, ce dernier a l'avantage de prendre en considération la dépendance entre certaines observations, dépendance provenant du fait de leur proximité qui engendre des caractéristiques similaires qui ne sont pas mesurées et considérées comme variables explicatives^[p. 123, Notes de cours du cours STT-7125]. Le modèle mixte a, donc, l'avantage de considérer la corrélation entre les observations et, ainsi, modéliser adéquatement les résidus. La forme du modèle est tel que : 

$$\boldsymbol{Y} = \boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{Z}\boldsymbol{\gamma} + \boldsymbol{\epsilon}$$

Où $\boldsymbol{\gamma}\sim N_K(0, D)$ et $D$ est la matrice de variance-covariance entre les observations.

Dans notre cas, nous avons identifié qu'il existe une dépendance non pas de la localisation, mais plutôt au travers les rivières et dans quel bassin elle se jettent. Ainsi, deux rivières qui se déversent dans le même bassin seront considérées dépendantes au travers les effets aléatoires de notre modèle. Ainsi, on obtient :

$$\boldsymbol{\gamma}_{\text{main_riv}} = \begin{bmatrix}
\gamma_{\text{main_riv}, 1}\\
\gamma_{\text{main_riv}, 2}\\
... \\
\gamma_{\text{main_riv}, 3844}\\
\end{bmatrix}_{3844 \times 1}$$

Si nous supposons que les 3 premières observations sont liées, respectivement, aux rivières #1, #1 et #3, nous obtenons la matrice $\boldsymbol{Z}$: 

$$\boldsymbol{Z} = \begin{bmatrix}
1 ~ 0~  0 ~ ... ~ 0\\
1 ~ 0 ~ 0~ ... ~ 0\\
1 ~ 0 ~ 1~ ... ~ 0\\
... \\
0 ~ 0 ~ 0~ ... ~ 0\\
\end{bmatrix}_{N \times 3844}$$

Puis, nous avons, finalement, la matrice $\boldsymbol{D}$. Cette dernière peut prendre plusieurs formes et nous allons tester dans notre analyse ces différentes formes possibles :

- Ordonnée à l'origine aléatoire
- Pente aléatoire pour différentes variables

### Choix des métriques

Notre variable d'intérêt, soit le débit d'eau des rivières, est une variable continue. De ce fait, nous avons la possibilité de prendre comme métrique de performance : 

- Erreur quadratique moyen (EQM)
- Erreur absolue moyen (EAM)

Nous prendrons l'erreur quatratique moyen autant pour évaluer nos modèles comme métrique à minimiser lorsque nous optimisons les hyperparamètres pour certains modèles qui en ont. Nous favorisons l'erreur quadratique moyenne pour mettre plus d'importance à minimiser les valeurs extrêmes. Le modèle ne vise pas seulement à prédire les débits élevés, mais nous portons une certaine importance. 

### Stratégie de validation et choix des hyperparamètres

Notre stratégie de validation est de séparer notre jeu de données en 3 parties soit une pour l'entraînement (70%), une pour la validation des modèles et leur comparaison entre eux (15%), et une pour tester notre modèle final (15%). Pour faire la sélection d'hyperparamètres, nous faisons une validation croisée à partir  du jeu d'entraînement. Pour estimer l'erreur de généralisation basée sur ces hyperparamètres optimaux, nous allons utiliser le jeu de validation. La raison pourquoi nous n'estimons pas l'erreur de généralisation sur notre jeu d'entraînement (et pourquoi nous avons ajouté un jeu de validation) est que si nous estimons l'erreur sur le jeu d'entraînement avec des hyperparamètre qui ont été choisis pour l'optimiser, nous allons possiblement sous-estimer notre erreur de généralisation puisque nous estimons une erreur avec des hyperparamètres qui la minimise. Ainsi, pour avoir un portrait adéquat de l'erreur de généralisation, nous utilisons le jeu de validation pour faire cette estimation. 

Par ailleurs, bien que nous avons une dépendance spatiale entre les observations, nous n'allons pas user de la validation croisée par bloc pour tenir en compte cette dépendance ou encore séparer notre jeu de données en 3 de façon à faire en sorte que les observations qui ont des dépendances entre elles se trouvent dans le même jeu de données . En effet, tel que spécifier dans l'article _Cross-validation strategies for data with temporal, spatial, hierarchical, or phylogenetic structure_^[p.917, mettre la référence], lorsqu'un modèle est créé dans le but de faire des prédictions dans les mêmes territoires (donc, la même structure de dépendance) que ceux vus par le modèle en entraînement, il n'est pas recommandé d'utiliser une telle stratégie de validation, car elle peut mener à biaiser l'estimation de l'erreur de généralisation. En effet, une validation croisée par bloc ou une séparation du jeu de données considérant la dépendance aura comme effet de venir surestimer l'estimation de l'erreur de généralisation, comme elle viendra estimer l'effet de faire des prédictions sur de nouveaux territoires, ce qui n'est pas l'usage désiré de ce modèle. 

### Préselection de variables 

Comme mentionné, pour modéliser le débit d'eau des rivières, nous allons utiliser un modèle mixte. Une caractéristique importante à soulever concernant ce modèle est qu'il a la caractéristique de ne pas bien tolérer la haute dimensionnalité ainsi que la multicolinéarité. Ainsi, comme notre jeu de données contient plusieurs variables (~50), nous faisons, avant l'étape de modélisation, une préselection de variables. En faisant ainsi, cela nous permet d'obtenir un modèle final plus interprétable et de diminuer les risques de surapprentissage^[Gregorutti, B. & Michel, B & Saint-Pierre, P. (2017). Correlation and variable importance in random forests. Statistics and Computing, v27 n3 (201705): 659-678.]. 

Pour ce faire, nous entraînons une forêt aléatoire pour faire une sélection de variables basée sur leur importance. Par la suite, nous faisons une analyse des composantes principales pour orthogonaliser les variables explicatives et, ainsi, nous défaire de la multicolinéarité. Nous allons tenter de trouver des interactions à possiblement ajouter à notre modèle final en faisant une arbre de décision. Puis, nous entraînons un modèle avec régularisation, soit un modèle de type _Lasso_, pour finaliser cette sélection de variables.

Il est à noter que les étapes de présélection de variables qui font appel à la variable d'intérêt se font sur les jeux d'entraînement et de validation seulement. En effet, comme il est mentionné, dans _Element of Statistical Learning_ (_ELS_)^[The elements of statistical learning : data mining, inference, and prediction, p. 245], l'objectif, dans une stratégie de validation, est d'avoir un jeu de données de test qui est indépendant du modèle que l'on entraîne. Or, si nous faisons la sélection des variables explicatives, sur l'ensemble de notre jeu de données, cette indépendance ne sera pas respectée puisque le modèle sera basé sur des variables explicatives qui ont été sélectionnées partiellement à partir du jeu de test. 

Par ailleurs, nous soulignons que l'ensemble des modèles construits pour faire la présélection sont évalués en terme de performance. En effet, même si un modèle entraîné n'a pas de bonnes performances, nous avons malgré tout avoir une importance des variables ou, dans le cas du modèle _Lasso_, des coefficients contractés à 0. De ce fait, avant de faire la sélection des variables, nous allons nous assurer que le modèle construit est un modèle qui est assez performant pour l'utiliser pour faire une sélection de variable à partir de celui-ci. L'étape d'évaluation des modèles se fera sur notre jeu de données de validation.

#### Diagnostique de multicolinéarité

Avant de faire une présélection des variables à partir de la forêt aléatoire, nous venons retirer les variables qui sont fortement corrélées. En effet, l'importance des variables peut être affecté par la multicolinéarité. Dans l'article _Correlation and variable importance in random forests_^[Idem], on y souligne le risque dans un contexte de multicolinéarité d'obtenir des résultats d'importance des variables instables : une petite modification du jeu d'entraînement peut entraîner une changement complet des variables dites importantes. Dans notre jeu de données, certaines variables sont très corrélées. Nous avons, notamment, des mesures qui ont été prises à différents endroits du bassin versant qui ont pratiquement les mêmes valeurs. Nous avons, également, la situation où nous avons des prises de mesure pour différents mois subséquents qui sont quasi-parfaitement corrélés. De ce fait, nous retirons certaines variables basé sur un diagnostique de multicolinéarité. Les détails de ce diagnostique (chaque itération) se retrouve en [annexe](#multicolinearity). Ce diagnostique fait en sorte que nous retirons les variables suivantes : `r tolower(paste(translate_var(ind_cond$removed_vars, suffixe = FALSE), collapse = ", "))`.

#### Forêt aléatoire 

La forêt aléatoire entraînée pour la préselection a été construite à partir de la librairie R _ranger_ (en [annexe](#random_forest) se retrouve les détails sur la méthodologie et les performances du modèl). L'importance des variables est obtenu à partir de la permutation des variables explicatives du jeu de données _Out of the Bag_ (OOB). Comme décrit dans _ELS_^[The elements of statistical learning : data mining, inference, and prediction, p. 593], l'idée générale est que, pour chaque arbre, nous avons un jeu de donnée laissé à part (OOB) que nous utilisons pour calculer, pour chaque variable explicatives, l'augmentation de l'erreur ou la diminution de l'exactitude entre les prédictions du jeu de données non-modifié et celles du jeu de données où nous permutons les valeurs de la variable évaluée. À chaque itération, en venant calculer l'impact sur un jeu de données indépendant des données vues par l'arbre entraîné, nous avons un portrait adéquat des variables importantes en généralisation, et non, importantes pour entraîner le modèle. Dans la librairie _ranger_, l'importance des variables est calculé à partir de la méthode décrite dans l'article _A computationally fast variable importance test for random forests for high-dimensional data. Adv Data Anal Classif_^[Janitza, S., Celik, E. & Boulesteix, A.-L., (2015). A computationally fast variable importance test for random forests for high-dimensional data. Adv Data Anal Classif doi: 10.1007/s116340160276- 4]. Mentionner ça : https://stats.stackexchange.com/questions/141619/wont-highly-correlated-variables-in-random-forest-distort-accuracy-and-feature

<span style="color:red">Quelle mesure de performance pour l'importance? Comment l'interpréter? À ajouter pour trouver les limites possibles</span>

Nous affichons les `r (nb_var <- 25)` variables les plus importantes basée sur la méthode précédemment décrite :

```{r}


# Afficher les variables importantes
show_imp_var(rf_model$model, nb_var = nb_var)

# Nombre de variables conservées

rf_nb_vars <- length(which(rf_model$model$variable.importance > 5))


```

Pour sélectionner les variables, plusieurs options s'offrent à nous. Nous pouvons sélectionner soient un nombre fixé de variables explicatives. Également, nous pouvons couper à partir d'un certain seuil d'importance de variable. Nous avons décidé de garder que les variables qui ont un niveau d'importance supérieur à 5 ce qui nous a mené à conserver `r rf_nb_vars` variables.

#### Analyse des composantes principales

Nous faisons, donc, une analyse des composantes principales avec la fonction `PCA` de la librairie `FactoMineR`. Celle-ci standardise, par défaut, nos variables. <span style="color:red"> Pourquoi on a nous-même standardisée? </span> L'objectif de cette analyse des composantes principales est d'orthogonaliser nos variables pour ensuite les utiliser comme variables explicatives pour le modèle mixte. Nous faisons l'analyse des composantes principales sur l'ensemble du jeu de données comme cette procédure ne fait pas appel à la variable d'intérêt.

```{r}


eig_dt <- as.data.table(pca$eig, keep.rownames = TRUE)
names(eig_dt) <- c("comp", "eigenvalue", "pourc_var", "cum_pouc_var")
w <- which(eig_dt[order(cum_pouc_var)][["cum_pouc_var"]] > 80)


```

Nous choisissons le nombre de composantes que nous conservons. Pour ce, nous appliquons la règle du 80%, la règle de _Joliffe_ ainsi que celle de _Cattell_ :

<div style="display: flex; justify-content: center;">

<div style="padding-right: 80px;">

**Règle du 80%**

```{r}


# Afficher la variance cumulée expliquée par composante
graph_pca_var_cum(pca, threshold = 80)


```

</div>

<div style="padding-right: 80px;">

**Règle de _Joliffe_**

```{r}

apply_joliffe_rule(pca)

```

</div>

<div style="padding-right: 80px;">

**Règle de _Cattell_**

```{r}


# Afficher le graphique pour appliquer la règle de Cattell
graph_cattell(pca)


```

</div>

</div>

**Constats :**
À partir du graphique ci-dessous qui expose la règle du 80%, on voit qu'il faut pour expliquer 80% de la variabilité de nos données, conserver `r w[1]` premières composantes. Si nous utilisons la règle de _Joliffe_, qui conserve que les composantes ayant des valeurs propres > 0.7, qui est plus sévère que celle de _Kaiser_, il faut conserver les 9 premières composantes principales. Si nous utilisons plutôt la règle de Cattell, pour faire cette sélection, on observe que les valeurs propres semblent se stabiliser à partir de la composante 10. Nous pourrions, donc, garder que les 9 premières composantes. Pour être conservateurs, nous conservons, donc, les 9 premières composantes.

Nous allons afficher les trois premières composantes avec la variable de débit (qui n'a pas été inclut dans cette analyse des composantes principales). Celles-ci conservent à elles seules plus de 50% de la variabilité du jeu de données. Nous affichons que les `r (nb_var <- 10)` variables qui contribuent le plus à la variabilité de ces composantes.

<div style="display: flex; justify-content: center;">

<div style="padding-right: 80px;">

**Composantes #1 et #2**

```{r, fig.height=6, fig.width=6, fig.align="c"}


# Afficher la composante #1 et #2
graph_pca(pca, nb_var = nb_var) 


```

</div>

<div style="padding-right: 80px;">

**Composantes #2 et #3**

```{r, fig.height=6, fig.width=6, fig.align="c"}


# Afficher la composante #2 et #3
graph_pca(pca, components = c(2, 3), nb_var = nb_var)


```

</div>

</div>

#### Arbre de décision

Nous avons entraîné un arbre de décision pour sélectionner les interactions les plus importantes à tester. Pour ce faire, nous avons fixé l'arbre de décision a une profondeur de 5 pour que ça reste interprétable. Puis, nous avons optimisé le reste des hyperparamètres par validation croisée sur le jeu d'entraînement (pour plus de détails sur la méthodologie et les performances du modèle, voir la section en [annexe](#decision_tree)). Nous voyons que l'arbre dans les premiers niveaux utilise très fréquement la variable `r translate_var("riv_tc", suffixe = FALSE)` et, dans les niveaux suivants, l'`r translate_var("ria_ha", suffixe = FALSE)` ressort souvent. Nous allons tester dans le modèle _Lasso_ l'interaction entre ces deux variables. Des interactions qui arrivent qu'une fois que nous allons tester, également, sont : 

- `r translate_var("ria_ha", suffixe = FALSE)`  + `r translate_var("riv_cross_area", suffixe = FALSE)`
- `r translate_var("ria_ha", suffixe = FALSE)`  + `r translate_var("riv_larg", suffixe = FALSE)`

Pour voir l'arbre obtenu, voir l'[annexe](#decision_tree).

#### Modèle _Lasso_

Nous utilisons un modèle _Lasso_ pour compléter notre sélection de variables. Ce modèle, étant donné la contrainte sur la somme de ses coefficients, a l'avantage de venir de contracter certains coefficients à 0 et faire, par ricochet, une sélection de variables. Par ailleurs, le modèle a l'avantage de gérer les situations où nous avons deux variables fortement corrélées. En effet, dans une régression normale avec deux variables très corrélées, l'un pourrait recevoir un coefficient positif élevé et l'autre un coefficient négatif pour capturer l'effet commun aux deux variables.^[he elements of statistical learning : data mining, inference, and prediction, p. 63]. Dans un modèle _Lasso_, le coefficient de l'une de ces 2 variables sera mis à 0 éliminant l'une des variables et gérant la situation de colinéarité. En [annexe](#lasso), nous décrivons plus en détail ce modèle, notre méthodologie pour entraîner le modèle et la performance de ce dernier.

Avant de faire ce modèle, nous avons fait une modèle additif généralisé (_GAM_) pour ajouter au modèle _Lasso_ certaines transformations polynomiales. Le modèle Lasso a conservé 3 transformation polynomiales de degré 2 pour les variables  : `r tolower(paste(translate_var(lasso_model$var_poly), collapse = ", "))`. Pour plus de détails, se réfèrer à l'[annexe](#lasso). De plus, le modèle a sélectionné l'interaction : `r translate_var("riv_tc", suffixe = FALSE)` x l'`r translate_var("ria_ha", suffixe = FALSE)`.

Voici, le résulats de la validation croisée pour obtenir le $\lambda$ optimal pour minimiser l'EQM ainsi que les variables sélectionnés par le modèle avec un tel $\lambda$.

<div style="display: flex; justify-content: center;">

<div style="padding-right: 80px;">

**Validation croisée**

```{r}

lasso_model$graph_cv

```

</div>

<div style="padding-right: 80px;">

**Variables conservées**

```{r}

coefs <- coef(
   object = lasso_model$model$fit, 
   s = lasso_model$model$spec$args$penalty
)

vars <- translate_var(coefs@Dimnames[[1]][coefs@i + 1][-1])
vars <- unique(gsub(":", "", sapply(vars, function(x) substr(x, 1, nchar(x) - 4))))

lasso_nb_vars <- length(vars)

vars %>% 
   kbl(
      align = "c", 
      digits = 2,
      col.names = "Variables conservées",
      booktabs = TRUE, 
      escape = FALSE
   ) %>% 
   kable_classic(
      full_width = FALSE, 
      html_font = "Cambria"
   )

```

</div>

</div>

Nous pouvons observer qu'en dessous d'une certaine valeur, nous obtenons toujours le même $\sqrt{EQM}$. Comme nous visions à sélectionner des variables avec ce modèle, nous avons pris le $\lambda$ le plus élevé au travers ceux qui ont le plus faible EQM (voir la ligne verticale). Ainsi, nous avons sélectionné un $\lambda$ = `r lasso_model$model$spec$args$penalty`. Avec ce dernier, nous passons, donc, de `r rf_nb_vars` variables à `r lasso_nb_vars`.

#### Modèle mixte

# Analyse, discussion et conclusion (2-3 pages)

## Performance des modèles


## Enjeux éthiques


## Recommandation du modèle? 

## Annexe

### A.1 Tableaux, cartes et graphiques pour l'analyse descriptive

Tableau 2.1.1 indiquant le nombre et le pourcentage de données manquantes par variable

```{r}

# Calcul du pourcentage par variable -------------------------------------------

# Calculer le nombre de données manquantes par variable et conserver que celles avec des données manquantes
var_missing_data <- unlist(lapply(
      X = river_dt, 
      FUN = function(x) {
         res <- sum(x == -999)
         if(res != 0) {res} else {NULL}
      }
   ))

# Changer les noms des variables 
names(var_missing_data) <- translate_var(names(var_missing_data))

# Changer le format de la table et le mettre en data.table
var_missing_data <- as.data.table(t(t(var_missing_data)), keep.rownames = TRUE)

# Calculer le % de données manquantes
var_missing_data[, pourc := paste0(round(V1 / nrow(river_dt) * 100, 1), "%")]

kbl(
   x = as.data.table(t(t(var_missing_data)), keep.rownames = TRUE), 
   align = c("l", "c", "c"), 
   col.names = c("Variables", "Nombre de données manquantes", "Pourcentage de données manquantes"), 
   caption = "Tableau 2.1.1 - Pourcentage de données manquantes par variable"
) %>% kable_classic(
   full_width = TRUE, 
   html_font = "Cambria"
)

```

Tableau 2.1.2 Jusitifant de la forte corrélation entre les variables _cav_ et _uav_

```{r, echo = FALSE}

# Nous remplaçons les -999 par des NA
col_w_na <- get_col_w_na(river_dt, na_pattern = -999)
river_dt[, (col_w_na) := 
            lapply(
               X = .SD, 
               FUN = function(x) fifelse(x == -999, get_na_by_type(x), x)),
         .SDcols = col_w_na]

# Calculer les corrélation entre les mêmes variables, mais cav et uav
var_suffix <- 
   unique(
      sapply(
         grep("uav", col_w_na, value = TRUE), 
         function(x) substr(x, 1, nchar(x) - 4)
      )
   )

res <-
   sapply(var_suffix, function(nm)
      round(cor(
         river_dt[[paste0(nm, "_cav")]],
         river_dt[[paste0(nm, "_uav")]],
         use = "complete.obs"),
         2
      ))

kbl(
   x = t(as.data.table(res)), 
   align = rep("c", length(res)),  
   col.names = translate_var(names(res)), 
   caption = "Tableau 2.1.2 - Coefficient de corrélation entre les variables étiquetées cav et uav, pour différentes métriques"
) %>% kable_classic(
   full_width = TRUE, 
   html_font = "Cambria"
)

# Nous retirons les variables de notre jeu de données


river_dt <- river_dt[, !grep("cav", col_w_na, value = TRUE), with = FALSE]

```

Tableau 2.1.3 décrivant les patrons de non-réponse de nos données manquantes 

```{r}

# Patrons de non-réponse -------------------------------------------------------

# Sélection des variables avec des valeurs manquantes
n <- get_col_w_na(river_dt)

# Création de la matrice R
R_dt <- as.data.table(is.na(river_dt[, n, with = FALSE]))

# Traduire les variables en leur description
names(R_dt) <- translate_var(names(R_dt))
n <- translate_var(n)

# Changer les -999 pour le nom de la variable manquante
R_dt[, (n) := lapply(n, function(var_name) {fifelse(R_dt[[var_name]], paste0("\U2022 ", var_name, " <br> "), "")})]

# Créer une colonne contenant l'ensemble des variables (patron) pour une observation
# donnée (une ligne) contenant des valeurs manquantes
R_dt[, pattern := apply(.SD, 1, function(x) paste(x, collapse = "")), .SDcols = n]

# Calculer le % de cas ayant chaque patron de valeurs manquantes
R_dt <- R_dt[pattern != "", paste0(round(.N / nrow(R_dt) * 100, 2), "%"), by = pattern]

R_dt <- cbind(data.table(num = 1:nrow(R_dt)), 
              R_dt)

R_dt %>%  knitr::kable(
   escape = FALSE,
   format = "html", 
   align = c("c", "l", "c"),
   col.names = c("Numéro", 
                 "Patrons de non-réponse", 
                 "Pourcentage de données manquantes"),
   caption = "Tableau 2.1.3 - Pourcentage de données manquantes par patron de non-réponse"
) %>% kable_classic(
   full_width = TRUE, 
   html_font = "Cambria"
) %>% row_spec(
   row = 1, 
   extra_css = "border-bottom: 1px solid")

```

Carte interactive utilisée en vérification des informations données sur les données manquantes par la documentation

```{r}
river_vec <- terra::vect(
   shape_file_path,
   extent = ext(c(-79.85, -55.53, 45.04, 62.80))
)
river_vec <- sf::st_as_sf(river_vec)
leaflet(river_vec[river_vec$slt_pc_uav == -999, ]) %>%
   addTiles() %>%
   addPolylines()

```

Graphiques justifiant que les données manquantes ne sont pas Missing Completely At Random

```{r}
river_dt[, patron := 0]
river_dt[is.na(snd_pc_uav), patron := 1]
```

::: columns 

:::: column

```{r}
graph_density(river_dt, var = "dis_m3_pyr", color = "patron")
```

On voit que la distribution du débit d'eau moyen pour les observations manquantes ont beaucoup plus de poids dans les extrêmes et possède une moyenne largement plus grande.

::::

:::: column

```{r}
graph_density(river_dt, var = "lka_pc_use", color = "patron")
```
On voit que pour les observations manquantes semblent se retrouver dans des territoires avec un plus haut pourcentage de terre correspondant à des lacs supposant.

::::

:::

::: columns

:::: column

```{r}
graph_density(river_dt, var = "for_pc_use", color = "patron")
```
Les moyennes du pourcentage occupé par les forêts sont relativement similaires. Un _test-t_ nous permettrait de valider si elles sont significativement différente. Par contre, la distribution est différente avec les données sans données manquantes qui se retrouvent plus souvent dans les proportions élevées.


::::

:::: column

```{r}
graph_density(river_dt, var = "ero_kh_cav", color = "patron")
```

L'érosion des sols semble être moins élevée pour les données manquantes.

::::

:::


### A.2 Diagnostique de multicolinéarité {#multicolinearity}

Voici, un portrait des itérations faits pour éliminer certaines variables qui sont hautement corrélées. À chaque étape, nous retirons une des variables qui ont de haute valeur.

- Itération 1

```{r}
table_ind_cond(ind_cond$ind_cond_1)
```

- Itération 2

```{r}
table_ind_cond(ind_cond$ind_cond_2)
```

- Itération 3

```{r}
table_ind_cond(ind_cond$ind_cond_3)
```

- Itération 4

```{r}
table_ind_cond(ind_cond$ind_cond_4)
```

Nous arrêtons les itérations, ici, car à partir de cette itération, nous n'avons plus de variable à retirer selon ce diagnostique.

### A.3 Forêt aléatoire {#random_forest}

- **Méthodologie**

La méthodologie employée pour entraîner ce modèle est composée des étapes suivantes :

1. Séparer le jeu de données en 3 parties
2. Validation croisée à 3 plis à partir du jeu d'entraînement pour optimiser les hyperparamètres (dans le but de minimiser l'EQM) :

   - Nombre de variables explicatives considérées à chaque embranchement (1, 8, 15) - Sélectionné par validation croisée : `r rf_model$model$mtry`
   - Nombre d'arbres (100, 300, 500) - Sélectionné par validation croisée : `r rf_model$model$num.trees`
   - Minimum d'observations dans un noeud (100, 550, 1000) - Sélectionné par validation croisée : `r rf_model$model$min.node.size`

3. Entraînement du modèle à partir du jeu d'entraînement avec les hyperparamètres optimaux et évaluation du modèle à partir du jeu de validation

- **Résultats**

L'EQM obtenu pour notre jeu de validation et, en comparaison, notre jeu d'entraînement :

```{r}

res <- data.table(
   rmse_train = rf_model$rmse_train$.estimate[1],
   rmse_val = rf_model$rmse_val[[1]]$.estimate[1]
)

res %>% 
   kbl(
      align = "c", 
      digits = 3,
      col.names = c("EQM (entr.)", "EQM (val.)"),
      booktabs = TRUE, 
      escape = FALSE
   ) %>% 
   kable_classic(
      full_width = FALSE, 
      html_font = "Cambria"
   )

```

**Constats :** On voit que notre modèle ne semble pas être en surapprentissage. L'EQM est de `r round(rf_model$rmse_val[[1]]$.estimate[1], 2)` sur notre jeu de données de validation. Or, cela implique qu'en moyenne l'écart de nos prédictions avec la valeur observée est  $\sqrt{EQM}$ = ~`r round(sqrt(rf_model$rmse_val[[1]]$.estimate[1]), 2)`. Pour se donner une idée de l'étendue de notre variable d'intérêt transformée :

- Moyenne : `r round(mean(river_dt$dis_m3_pyr), 3)`
- Écart-type : `r round(sd(river_dt$dis_m3_pyr), 3)`

### A.4 Arbre de décision {#decision_tree}

- **Méthodologie**

1. Séparer le jeu de données en 3 parties
2. Validation croisée à 3 plis à partir du jeu d'entraînement pour optimiser les paramètres (le paramètre de profondeur est fixé à 5) :

   - Coût de complexité (0.0000000001, 0.00000316 et 0.1) - Sélection par validation croisée : `r tree_model$model$fit$control$cp` 
   - Minimum d'observations dans un noeud (100, 550, 1000) - Sélection par validation croisée : `r tree_model$model$fit$control$minsplit`
   
3. Entraînement du modèle à partir du jeu d'entraînement avec les hyperparamètres optimaux et évaluation du modèle à partir du jeu de validation

- **Résultats**

Nous obtenons l'arbre suivant :

```{r}

rpart.plot::rpart.plot(tree_model$model$fit, roundint = FALSE)

```

L'EQM obtenu pour notre jeu de validation et, en comparaison, notre jeu d'entraînement :

```{r}

res <- data.table(
   rmse_train = tree_model$rmse_train$.estimate[1],
   rmse_val = tree_model$rmse_val[[1]]$.estimate[1]
)

res %>% 
   kbl(
      align = "c", 
      digits = 3,
      col.names = c("EQM (entr.)", "EQM (val.)"),
      booktabs = TRUE, 
      escape = FALSE
   ) %>% 
   kable_classic(
      full_width = FALSE, 
      html_font = "Cambria"
   )

```

**Constats :** On voit que notre modèle ne semble pas être en surapprentissage. L'EQM est de `r round(tree_model$rmse_val[[1]]$.estimate[1], 2)` sur notre jeu de données de validation. Or, cela implique qu'en moyenne l'écart de nos prédictions avec la valeur observée est  $\sqrt{EQM}$ = ~`r round(sqrt(tree_model$rmse_val[[1]]$.estimate[1]), 2)`. On voit sans surprise que cette arbre de décision a surappris.

### A.5 Modèle _Lasso_ {#lasso}

Un modèle _Lasso_ est une méthode de contraction des coefficients. Comparé au modèle linéaire, ce modèle se voit ajouter une contrainte, soit une valeur maximale totale qu'il peut attribuer à l'ensemble de ses coefficients. Les coefficients du modèle sont établis en minimisant l'erreur quadratique au travers les observations avec la contrainte que la somme des coefficients ne peut pas dépasser une certaine valeur :

$$\underset{\vec{\beta}}{argmin}(\sum_{i=1}^N(y_i -\hat{y}_i)^2 + \lambda\sum_{j}^p|\beta_j|)$$

Comme le démontre cette formule, plus la valeur de lambda est élevé, plus la contrainte est grande, et plus le nombre de coefficients qui risquent d'être contractés à 0 est élevé. Pour déterminer ce seul et unique hyperparamètre du modèle, nous effectuons une validation croisée sur le jeu d'entraînement. 

- **Méthodologie**

1. Séparer le jeu de données en 3 parties
2. Sélectionner à partir d'un modèle additif généralisé des transformations possibles
3. Validation croisée à 3 plis à partir du jeu d'entraînement pour optimiser le paramètre de pénalité $\lambda$ (minimiser l'EQM) : 25 valeurs entre $1 \times 10^{-10}$ et 1.
4. Entraînement du modèle à partir du jeu d'entraînement avec les hyperparamètres optimaux et évaluation du modèle à partir du jeu de validation

- **Résultats**

Avant de regarder les résultats, nous validons les hypothèses qui sous-tendent le modèle soit : l'hypothèse de linéarité, l'hypothèse d'homoscédasticité, l'hypothèse de non-corrélation et l'hypothèse de normalité des résidus. Évidemment, sachant qu'il y a de la dépendance spatiale entre certaines rivières, nous nous attendons que l'hypothèse d'homoscédasticité et de non-corrélation ne soient pas respectées.

<div style="display: flex; justify-content: center;">

<div style="padding-right: 80px;">

**Hypothèses de linéarité et d'homoscédasticité**

```{r}

lasso_model$graphs_res$res_vs_pred

```

</div>

<div style="padding-right: 80px;">

**Hypothèse de non-corrélation**

```{r}

lasso_model$graphs_res$res_vs_etiq

```

</div>

<div style="padding-right: 80px;">

**Hypothèse de normalité des résidus**

```{r}

lasso_model$graphs_res$res_qqplot

```

</div>

</div>
**Constats :** On voit que l'hypothèse d'homoscédasticité est clairement pas respectée. Étant donné le fait que nous avons une structure de corrélation, l'allure de ce graphique  n'est pas suprenant et sera possiblement corrigé avec un modèle mixte. La non-corrélation que nous permet de voir le 2e graphique est attendu vu cette même structure de dépendance. Pour ce qui est du troisième graphique, on voit qu'il y a une certaine déviation des résidus de la normalité. Cette dernière n'est pas dramatique et sera possiblement réglé par le modèle mixte.


Voici, l'EQM obtenu pour notre jeu de validation et, en comparaison, celui avec basé sur le jeu d'entraînement :

```{r}

res <- data.table(
   rmse_train = lasso_model$rmse_train$.estimate[1],
   rmse_val = lasso_model$rmse_val[[1]]$.estimate[1]
)

res %>% 
   kbl(
      align = "c", 
      digits = 3,
      col.names = c("EQM (entr.)", "EQM (val.)"),
      booktabs = TRUE, 
      escape = FALSE
   ) %>% 
   kable_classic(
      full_width = FALSE, 
      html_font = "Cambria"
   )

```
**Constats :** L'EQM est de `r round(lasso_model$rmse_val[[1]]$.estimate[1], 2)` sur notre jeu de données de validation. Or, cela implique qu'en moyenne l'écart de nos prédictions avec la valeur observée est  $\sqrt{EQM}$ = ~`r round(sqrt(lasso_model$rmse_val[[1]]$.estimate[1]), 2)`. On remarque que l'on a un EQM plus élevé qu'avec la forêt aléatoire. Toutefois, rappelons que bien que le modèle que nous allons proposer est peut-être moins performant, le modèle est beaucoup plus interprétable.

### A.5 Modèle mixte {#mixte}

**Méthodologie** :
1. Identification des effets aléatoires à tester
À partir du modèle _Lasso_ qui ne contient aucun effet aléatoire, nous avons regardé les graphiques des résidus par rapport à différentes variables pour avoir un indication si des effets aléatoires seraient nécessaires, et ce, au travers la structure de dépendance basée sur la rivière commune en aval. Nous observons avec les graphiques suivants qu'un ordonnée à l'origine aléatoire ainsi que des pentes aléatoires pour les variables _ria_ha_csu_ (aire de la rivière) et _riv_tc_usu_ (volume de la rivière) seraient pertinente :

```{r}

info_ls$graphs_res$res_vs_var$ria_ha_csu

```


2. Choix de la structure de covariance
Nous testons plusieurs options :
- C




