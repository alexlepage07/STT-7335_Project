---
title: "Projet"
output: 
  html_document:
    css: style.css
---

```{r, include = FALSE}

# Paramètres pour le .Rmd ------------------------------------------------------


knitr::opts_chunk$set(
   echo = FALSE,
   align = "c"
)


# Chemins d'accès --------------------------------------------------------------


path_data <- "../Data"
path_obj <- "../inst"
path_fct <- "../R"


# Charger les objets -----------------------------------------------------------


rf_model <- readRDS(file.path(path_obj, "s5_random_forest.rds"))
pca <- readRDS(file.path(path_obj, "s6_pca.rds"))
river_dt <- readRDS(file.path(path_data, "s3_donnees_standardisees.rds"))


# Charger les fonctions utiles -------------------------------------------------


source(file.path(path_fct, "utils.R"))

```

# Contexte et objectifs (1/2 page)

- Contexte
- Objectifs
- Hypothèses

# Préparation et nettoyage des données (1 page)

- Source
- Description
- Type de variables
- Vérifier les doublons
- Vérifier les erreurs de saisie
- Uniformisation des valeurs
- Validation de fusion de tables

# Analyse descriptive et exploratoire (2-3 pages)

- Type d’échantillon
- Nombre d'observations et variables
- Analyse descriptive uni et multivariées
- Identification des valeurs extrêmes et aberrantes (+ traitées)
- Gestion des données manquantes 
Doit se faire sur la jeu de données d'entraînement (voir p.245 ESL)
- Création de nouvelles variables
- Identification de la dépendance

# Modélisation et validation (1-2 pages)

## Pertinence du modèle

## Choix des métriques

Notre variable d'intérêt, soit le débit d'eau des rivières, est une variable continue. De ce fait, nous avons la possibilité de prendre comme métrique de performance : 

- Erreur quadratique moyen (EQM)
- Erreur absolue moyen (EAM)
- Gini <span style="color:red">Il existe il me semble le Gini pour des variables continues</span>

Nous prendrons l'erreur quatratique moyen autant pour évaluer nos modèles que comme métrique à minimiser lorsque nous optimisons les hyperparamètres pour certains modèles qui en ont.

<span style="color:red">Justifier pourquoi on ne prendre pas EAM</span>

## Choix des hyperparamètres 

## Stratégie de validation

Notre stratégie de validation est de séparer notre jeu de données en 3 parties soit une pour l'entraînement (70%), une pour la validation des modèles et leur comparaison entre eux (15%), et une pour tester notre modèle final (15%). Pour faire la sélection d'hyperparamètres, nous faisons une validation croisée à partir  du jeu d'entraînement. Pour estimer l'erreur de généralisation basée sur ces hyperparamètres optimaux, nous allons utiliser le jeu de validation. La raison pourquoi nous n'estimons pas l'erreur de généralisation sur notre jeu d'entraînement (et pourquoi nous avons ajouté un jeu de validation) est que si nous estimons l'erreur sur le jeu d'entraînement avec des hyperparamètre qui ont été choisis pour l'optimiser, nous allons possiblement sous-estimer notre erreur de généralisation puisque nous estimons une erreur avec des hyperparamètres qui la minimise. Ainsi, pour avoir un portrait adéquat de l'erreur de généralisation, nous utilisons le jeu de validation pour faire cette estimation. 

Par ailleurs, bien que nous avons une dépendance spatiale entre les observations, nous n'allons pas user de la validation croisée par bloc pour tenir en compte cette dépendance ou encore séparer notre jeu de données en 3 de façon à faire en sorte que les observations qui ont des dépendances entre elles se trouvent dans le même jeu de données . En effet, tel que spécifier dans l'article _Cross-validation strategies for data with temporal, spatial, hierarchical, or phylogenetic structure_^[p.917, mettre la référence], lorsqu'un modèle est créé dans le but de faire des prédictions dans les mêmes territoires (donc, la même structure de dépendance) que ceux vus par le modèle en entraînement, il n'est pas recommandé d'utiliser une telle stratégie de validation, car elle peut mener à biaiser l'estimation de l'erreur de généralisation. En effet, une validation croisée par bloc ou une séparation du jeu de données considérant la dépendance aura comme effet de venir surestimer l'estimation de l'erreur de généralisation, comme elle viendra estimer l'effet de faire des prédictions sur de nouveaux territoires, ce qui n'est pas l'usage désiré de ce modèle. 

## Préselection de variables 

Comme mentionné, pour modéliser le débit d'eau des rivières, nous allons utiliser un modèle mixte. Une caractéristique importante à soulever concernant ce modèle est qu'il a la caractéristique de ne pas bien tolérer la haute dimensionnalité ainsi que la multicolinéarité (<span style="color:red">mettre une référence</span>). Ainsi, comme notre jeu de données contient plusieurs variables (~50), nous faisons, avant l'étape de modélisation, une préselection de variables. En faisant ainsi, cela nous permet d'obtenir un modèle final plus interprétable et de diminuer les risques de surapprentissage^[Gregorutti, B. & Michel, B & Saint-Pierre, P. (2017). Correlation and variable importance in random forests. Statistics and Computing, v27 n3 (201705): 659-678.]. Pour ce faire, nous entraînons une forêt aléatoire pour faire une sélection de variables basée sur leur importance. Par la suite, nous faisons une analyse des composantes principales pour orthogonaliser les variables explicatives et, ainsi, nous défaire de la multicolinéarité. Puis, nous entraînons un modèle avec régularisation, soit un modèle de type _Lasso_, pour finaliser cette sélection de variables. Ce sera aprèes ces étapes de présélection de variables que nous ferons la dernière étape qui est de modéliser notre variable d'intérêt avec un modèle mixte.

#### Diagnostique de multicolinéarité

Avant de faire une présélection des variables à partir de la forêt aléatoire, nous venons retirer les variables qui sont fortement corrélées. En effet, l'importance des variables peut être affecté par la multicolinéarité. Dans l'article _Correlation and variable importance in random forests_^[Idem], on y souligne le risque dans un contexte de multicolinéarité d'obtenir des résultats d'importance des variables instables : une petite modification du jeu d'entraînement peut entraîner une changement complet des variables dites importantes. Dans notre jeu de données, certaines variables sont très corrélées. Nous avons, notamment, des mesures qui ont été prises à différents endroits du bassin versant qui ont pratiquement les mêmes valeurs. Nous avons, également, la situation où nous avons des prises de mesure pour différents mois subséquents qui sont quasi-parfaitement corrélés. De ce fait, nous retirons certaines variables basé sur notre diagnostique de multicolinéarité.

#### Forêt aléatoire 

Il est à noter que cette étape de présélection avec une forêt aléatoire se fait sur le jeu d'entraînement seulement. En effet, comme il est mentionné, dans _Element of Statistical Learning_ (_ELS_)^[The elements of statistical learning : data mining, inference, and prediction, p. 245], l'objectif, dans une stratégie de validation, est d'avoir un jeu de données de test qui est indépendant du modèle que l'on entraîne. Or, si nous faisons la sélection des variables explicatives, sur l'ensemble de notre jeu de données, cette indépendance ne sera pas respectée puisque le modèle sera basé sur des variables explicatives qui ont été sélectionnées partiellement à partir du jeu de test. Par ailleurs, nous soulignons que l'ensemble des modèles construits pour faire la présélection sont évalués en terme de performance. En effet, même si un modèle entraîné n'a pas de bonnes performances, nous avons toujours quand même une importance des variables ou, dans le cas du modèle Lasso, des coefficients contractés à 0. De ce fait, avant de faire la sélection des variables, nous allons nous assurer que le modèle construit est un modèle qui est assez performant pour l'utiliser pour faire une sélection de variable. L'étape d'évaluation des modèles se fera sur notre jeu de données de validation.

La forêt aléatoire entraînée pour la préselection a été construite à partir de la librairie R _ranger_ (pour avoir de détails sur la méthodologie, se réfèrer à l'annexe). L'importance des variables est obtenu à partir de la permutation des variables explicatives du jeu de données _Out of the Bag_ (OOB). Comme décrit dans _ELS_^[The elements of statistical learning : data mining, inference, and prediction, p. 593], l'idée générale est que, pour chaque arbre, nous avons un jeu de donnée laissé à part (OOB) que nous utilisons pour calculer, pour chaque variable explicatives, l'augmentation de l'erreur ou la diminution de l'exactitude entre les prédictions du jeu de données non-modifié et celles du jeu de données où nous permutons les valeurs de la variable évaluée. À chaque itération, en venant calculer l'impact sur un jeu de données indépendant des données vues par l'arbre entraîné, nous avons un portrait adéquat des variables importantes en généralisation, et non, importantes pour entraîner le modèle. Dans la librairie _ranger_, l'importance des variables est calculé à partir de la méthode décrite dans l'article _A computationally fast variable importance test for random forests for high-dimensional data. Adv Data Anal Classif_^[Janitza, S., Celik, E. & Boulesteix, A.-L., (2015). A computationally fast variable importance test for random forests for high-dimensional data. Adv Data Anal Classif doi: 10.1007/s116340160276- 4]. Mentionner ça : https://stats.stackexchange.com/questions/141619/wont-highly-correlated-variables-in-random-forest-distort-accuracy-and-feature

<span style="color:red">Quelle mesure de performance pour l'importance? Comment l'interpréter? À ajouter pour trouver les limites possibles</span>

##### Méthodologie

- Diagnostique de multicolinéarité
- Séparer le jeu de données en 3 parties
- Validation croisée à 3 plis à partir du jeu d'entraînement pour optimiser les hyperparamètres (minimiser l'EQM) :
      - Nombre de variables explicatives considérées à chaque embranchement     (1, 8, 15)
      - Nombre d'arbres (100, 300, 500)
      - Minimum d'observations dans un noeud (100, 550, 1000)
- Entraînement du modèle à partir du jeu d'entraînement avec les hyperparamètres optimaux et évaluation de l'EQM sur le jeu de validation
- Sélection des 25 variables les plus importantes

##### Résultats

Voici, l'EQM obtenu pour notre jeu de validation et, en comparaison, notre jeu d'entraînement :

```{r}

res <- data.table(
   rmse_train = rf_model$rmse_train$.estimate[1],
   rmse_val = rf_model$rmse_val[[1]]$.estimate[1]
)

res %>% 
   kbl(
      align = "c", 
      digits = 2,
      col.names = c("EQM (entr.)", "EQM (val.)"),
      booktabs = TRUE, 
      escape = FALSE
   ) %>% 
   kable_classic(
      full_width = FALSE, 
      html_font = "Cambria"
   )

```
**Constats :** On voit que notre modèle ne semble pas être en surapprentissage. L'EQM est de 0.24 sur notre jeu de données de validation. Or, cela implique qu'en moyenne l'écart de nos prédictions avec la vraie value est ~0.5 ($\sqrt{EQM}$). Pour se donner une idée de l'étendue de notre variable d'intérêt transformée :

- Moyenne : `r round(mean(river_dt$dis_m3_pyr), 3)`
- Écart-type : `r round(sd(river_dt$dis_m3_pyr), 3)`


Nous affichons les `r (nb_var <- 25)` variables les plus importantes basée sur la méthode précédemment décrite :

```{r}


# Afficher les variables importantes
show_imp_var(rf_model$model, nb_var = nb_var)


```

**Constats :**

Pour sélectionner les variables, plusieurs options s'offrent à nous. Nous pouvons sélectionner soient un nombre fixé de variables explicatives. Également, nous pourrions couper à partir d'un certain seuil d'importance de variable. 

#### Analyse des composantes principales

Nous faisons, donc, une analyse des composantes principales avec la fonction `PCA` de la librairie `FactoMineR`. Celle-ci standardise, par défaut, nos variables. <span style="color:red"> Pourquoi on a nous-même standardisée? </span> L'objectif de cette analyse des composantes principales est d'orthogonaliser nos variables pour ensuite les utiliser comme variables explicatives pour le modèle mixte. Nous faisons l'analyse des composantes principales sur l'ensemble du jeu de données comme cette procédure ne fait pas appel à la variable d'intérêt.

```{r}
eig_dt <- as.data.table(pca$eig, keep.rownames = TRUE)
names(eig_dt) <- c("comp", "eigenvalue", "pourc_var", "cum_pouc_var")
w <- which(eig_dt[order(cum_pouc_var)][["cum_pouc_var"]] > 80)
```

Nous choisissons le nombre de composantes que nous conservons. Pour ce, nous appliquons la règle du 80%, la règle de _Joliffe_ ainsi que celle de _Cattell_ :

<div style="display: flex; justify-content: center;">

<div style="padding-right: 80px;">

**Règle du 80%**

```{r}


# Afficher la variance cumulée expliquée par composante
graph_pca_var_cum(pca, threshold = 80)


```

</div>

<div style="padding-right: 80px;">

**Règle de _Joliffe_**

```{r}

apply_joliffe_rule(pca)

```

</div>

<div style="padding-right: 80px;">

**Règle de _Cattell_**

```{r}


# Afficher le graphique pour appliquer la règle de Cattell
graph_cattell(pca)


```

</div>

</div>

**Constats :**
À partir du graphique ci-dessous qui expose la règle du 80%, on voit qu'il faut pour expliquer 80% de la variabilité de nos données, conserver `r w[1]` premières composantes. Si nous utilisons la règle de _Joliffe_, qui conserve que les composantes ayant des valeurs propres > 0.7, qui est plus sévère que celle de _Kaiser_, il faut conserver les 8 premières composantes principales. Si nous utilisons plutôt la règle de Cattell, pour faire cette sélection, on observe que les valeurs propres semblent se stabiliser à partir de la composante 10. Nous pourrions, donc, garder que les 9 premières composantes. Pour être conservateurs, nous conservons, donc, les 9 premières composantes.

Nous allons afficher les trois premières composantes avec la variable de débit (qui n'a pas été inclut dans cette analyse des composantes principales). Celles-ci conservent à elles seules plus de 50% de la variabilité du jeu de données. Nous affichons que les `r (nb_var <- 10)` variables qui contribuent le plus à la variabilité de ces composantes.

<div style="display: flex; justify-content: center;">

<div style="padding-right: 80px;">

**Composantes #1 et #2**

```{r, fig.height=6, fig.width=6, fig.align="c"}


# Afficher la composante #1 et #2
graph_pca(pca, nb_var = nb_var)


```

</div>

<div style="padding-right: 80px;">

**Composantes #2 et #3**

```{r, fig.height=6, fig.width=6, fig.align="c"}


# Afficher la composante #2 et #3
graph_pca(pca, components = c(2, 3), nb_var = nb_var)


```

</div>

</div>

#### Modèle _Lasso_

Un modèle _Lasso_ est une méthode de contraction des coefficients. Comparé au modèle linéaire, ce modèle se voit ajouté une contrainte soit une valeur maximale totale qu'il peut attribuer à l'ensemble de ses coefficients. Les coefficients du modèle sont établis en minimisant l'erreur quadratique au travers les observations avec la contrainte que la somme des coefficients ne peut pas dépasser une certaine valeur :

$$\underset{\vec{\beta}}{argmin}(\sum_{i=1}^N(y_i -\hat{y}_i)^2 + \lambda\sum_{j}^p|\beta_j|)$$

Comme le démontre cette formule, plus la valeur de lambda est élevé, plus la contrainte est grande, et plus le nombre de coefficients qui sont contractés risquent d'être élevé. Pour déterminer ce seul et unique hyperparamètre du modèle, nous effectuons une validation croisée sur le jeu d'entraînement. 

Le modèle a l'avantage de gérer les situations où nous avons deux variables fortement corrélées où, dans une régression normale, l'un pourrait recevoir un coefficient positif élevé et l'autre un coefficient négatif pour capturer l'effet commun aux deux variables^[he elements of statistical learning : data mining, inference, and prediction, p. 63].


$$$$

# Analyse, discussion et conclusion (2-3 pages)

- Performance des modèles
- Enjeux éthiques
- Recommandation du modèle? 

# Annexe

