---
title: "Projet"
output: 
  html_document:
    css: style.css
---

```{r, include = FALSE}

# Paramètres pour le .Rmd ------------------------------------------------------


knitr::opts_chunk$set(
   echo = FALSE,
   align = "c"
)


# Chemins d'accès --------------------------------------------------------------


path_data <- "../Data"
path_obj <- "../inst"
path_fct <- "../R"


# Charger les objets -----------------------------------------------------------


rf_model <- readRDS(file.path(path_obj, "s5_random_forest.rds"))
pca <- readRDS(file.path(path_obj, "s6_pca.rds"))


# Charger les fonctions utiles -------------------------------------------------


source(file.path(path_fct, "utils.R"))

```

# Contexte et objectifs (1/2 page)

- Contexte
- Objectifs
- Hypothèses

# Préparation et nettoyage des données (1 page)

- Source
- Description
- Type de variables
- Vérifier les doublons
- Vérifier les erreurs de saisie
- Uniformisation des valeurs
- Validation de fusion de tables

# Analyse descriptive et exploratoire (2-3 pages)

- Type d’échantillon
- Nombre d'observations et variables
- Analyse descriptive uni et multivariées
- Identification des valeurs extrêmes et aberrantes (+ traitées)
- Gestion des données manquantes 
Doit se faire sur la jeu de données d'entraînement (voir p.245 ESL)
- Création de nouvelles variables
- Identification de la dépendance

# Modélisation et validation (1-2 pages)

- Pertinence du modèle
- Choix des hyperparamètres + stratégie de validation
- Identification d'interactions possibles
- Pré-sélection de variables
- Choix des métriques

## Pertinence du modèle

## Choix des hyperparamètres 

## Stratégie de validation

Notre stratégie de validation est de séparer notre jeu de données en 3 parties soit une pour l'entraînement (70%), une pour la validation des modèles et leur comparaison entre eux (15%), et une pour tester notre modèle final (15%). Pour faire la sélection d'hyperparamètres, nous faisons une validation croisée à partir  du jeu d'entraînement. Pour estimer l'erreur de généralisation basée sur ces hyperparamètres optimaux, nous allons utiliser le jeu de validation. La raison pourquoi nous n'estimons pas l'erreur de généralisation sur notre jeu d'entraînement (et pourquoi nous avons ajouté un jeu de validation) est que si nous estimons l'erreur sur le jeu d'entraînement avec des hyperparamètre qui ont été choisis pour l'optimiser, nous allons possiblement sous-estimer notre erreur de généralisation puisque nous estimons une erreur avec des hyperparamètres qui la minimise. Ainsi, pour avoir un portrait adéquat de l'erreur de généralisation, nous utilisons le jeu de validation pour faire cette estimation. 

Par ailleurs, bien que nous avons une dépendance spatiale entre les observations, nous n'allons pas user de la validation croisée par bloc pour tenir en compte cette dépendance ou encore séparer notre jeu de données en 3 de façon à faire en sorte que les observations qui ont des dépendances entre elles se trouvent dans le même jeu de données . En effet, tel que spécifier dans l'article _Cross-validation strategies for data with temporal, spatial, hierarchical, or phylogenetic structure_^[p.917, mettre la référence], lorsqu'un modèle est créé dans le but de faire des prédictions dans les mêmes territoires (donc, la même structure de dépendance) que ceux vus par le modèle en entraînement, il n'est pas recommandé d'utiliser une telle stratégie de validation, car elle peut mener à biaiser l'estimation de l'erreur de généralisation. En effet, une validation croisée par bloc ou une séparation du jeu de données considérant la dépendance aura comme effet de venir surestimer l'estimation de l'erreur de généralisation, comme elle viendra estimer l'effet de faire des prédictions sur de nouveaux territoires, ce qui n'est pas l'usage désiré de ce modèle. 

## Préselection de variables 

Comme mentionné, pour modéliser le débit d'eau des rivières, nous allons utiliser un modèle mixte. Une caractéristique importante à soulever concernant ce modèle est qu'il a la caractéristique de ne pas bien tolérer la haute dimensionnalité ainsi que la multicolinéarité (<span style="color:red">mettre une référence</span>). Ainsi, comme notre jeu de données contient plusieurs variables (~50), nous faisons, avant l'étape de modélisation, une préselection de variables. Pour ce faire, nous entraînons une forêt aléatoire pour faire une sélection de variables basée sur leur importance. Par la suite, nous faisons une analyse des composantes principales pour orthogonaliser les variables explicatives et, ainsi, nous défaire de la multicolinéarité. Nous allons finalement faire une modélisation avec régularisation, soit celle de type _Lasso_, pour finaliser cette sélection de variables. Nous passerons, ensuite, à la dernière étape qui est de modéliser notre variable d'intérêt avec un modèle mixte, et ce, avec les variables finales obtenues avec la régression _Lasso_.

- Forêt aléatoire 

Avant de faire une présélection des variables à partir de la forêt aléatoire, nous venons retirer les variables qui sont fortement corrélées. En effet, l'importance des variables peut être affecté par un contexte de multicolinéarité.<span style="color:red">Référence + développer</span> Dans notre jeu de données, certaines variables sont très corrélées. Nous avons, notamment, des mesures qui ont été prises à différents endroits du bassin versant qui ont pratiquement les mêmes valeurs. Nous avons, également, la situation où nous avons des prises de mesure pour des différents mois subséquents qui sont quasi-parfaitement corrélés. De ce fait, nous retirons certaines variables pour ne garder qu'une seule des deux qui ont des coefficient de corrélation $\rho_{i, j}>0.95$, et ce, dans le but de ne pas biaiser notre sélection de variables par la forêt aléatoire.

Il est à noter que cette étape de présélection se fait sur le jeu d'entraînement seulement. En effet, comme il est mentionné, dans _Element of Statistical Learning_ (_ELS_)^[The elements of statistical learning : data mining, inference, and prediction, p. 245], l'objectif, dans une stratégie de validation, est d'avoir un jeu de données de test qui est indépendant du modèle que l'on entraîne. Or, si nous faisons la sélection des variables explicatives, sur l'ensemble de notre jeu de données, cette indépendance ne sera pas respectée puisque le modèle sera basé sur des variables explicatives qui ont été sélectionnées partiellement à partir du jeu de test. 

La forêt aléatoire entraînée pour la préselection a été construite à partir de la librairie R _ranger_ (pour avoir de détails sur la méthodologie, se réfèrer à l'annexe). L'importance des variables est obtenu à partir de la permutation des variables explicatives du jeu de données _Out of the Bag_ (OOB). Comme décrit dans _ELS_^[The elements of statistical learning : data mining, inference, and prediction, p. 593], l'idée générale est que, pour chaque arbre, nous avons un jeu de donnée laissé à part (OOB) que nous utilisons pour calculer, pour chaque variable explicatives, l'augmentation de l'erreur ou la diminution de l'exactitude entre les prédictions du jeu de données non-modifié et celles du jeu de données où nous permutons les valeurs de la variable évaluée. À chaque itération, en venant calculer l'impact sur un jeu de données indépendant des données vues par l'arbre entraîné, nous avons un portrait adéquat des variables importantes en généralisation, et non, importantes pour entraîner le modèle. Dans la librairie _ranger_, l'importance des variables est calculé à partir de la méthode décrite dans l'article _A computationally fast variable importance test for random forests for high-dimensional data. Adv Data Anal Classif_^[Janitza, S., Celik, E. & Boulesteix, A.-L., (2015). A computationally fast variable importance test for random forests for high-dimensional data. Adv Data Anal Classif doi: 10.1007/s116340160276- 4]. Mentionner ça : https://stats.stackexchange.com/questions/141619/wont-highly-correlated-variables-in-random-forest-distort-accuracy-and-feature

Il est à noter que même si le modèle entraîné n'était pas bon, nous aurions quand même une importance de variables. De ce fait, avant de faire la sélection des variables, nous allons nous assurer que le modèle construit est un bon modèle.

<span style="color:red">Quelle mesure de performance pour l'importance? Comment l'interpréter? À ajouter pour trouver les limites possibles</span>
      
Nous affichons les `r (nb_var <- 20)` variables les plus importantes basée sur la méthode précédemment décrite :

```{r}


# Afficher les variables importantes
show_imp_var(rf_model$fit, nb_var = nb_var)


```

**Constats :**

Pour sélectionner les variables, plusieurs options s'offrent à nous. Nous pouvons sélectionner soient un nombre fixé de variables explicatives. Également, nous pourrions couper à partir d'un certain seuil d'importance de variable. 

- Analyse des composantes principales

Nous faisons, donc, une analyse des composantes principales avec la fonction `PCA` de la librairie `FactoMineR`. Celle-ci standardise, par défaut, nos variables. <span style="color:red"> Pourquoi on a nous-même standardisée? </span> L'objectif de cette analyse des composantes principales est d'orthogonaliser nos variables pour ensuite les utiliser comme variables explicatives pour le modèle mixte.

```{r}
eig_dt <- as.data.table(pca$eig, keep.rownames = TRUE)
names(eig_dt) <- c("comp", "eigenvalue", "pourc_var", "cum_pouc_var")
w <- which(eig_dt[order(cum_pouc_var)][["cum_pouc_var"]] > 80)
```

Nous choisissons le nombre de composantes que nous conservons. Pour ce, nous appliquons la règle du 80%, la règle de _Joliffe_ ainsi que celle de _Cattell_ :

<div style="display: flex; justify-content: center;">

<div style="padding-right: 80px;">

**Règle du 80%**

```{r}


# Afficher la variance cumulée expliquée par composante
graph_pca_var_cum(pca, threshold = 80)


```

</div>

<div style="padding-right: 80px;">

**Règle de _Joliffe_**

```{r}

apply_joliffe_rule(pca)

```

</div>

<div style="padding-right: 80px;">

**Règle de _Cattell_**

```{r}


# Afficher le graphique pour appliquer la règle de Cattell
graph_cattell(pca)


```

</div>

</div>

**Constats :**
À partir du graphique ci-dessous qui expose la règle du 80%, on voit qu'il faut pour expliquer 80% de la variabilité de nos données, conserver `r w[1]` premières composantes. Si nous utilisons la règle de _Joliffe_, qui conserve que les composantes ayant des valeurs propres > 0.7, qui est plus sévère que celle de _Kaiser_, il faut conserver les 8 premières composantes principales. Si nous utilisons plutôt la règle de Cattell, pour faire cette sélection, on observe que les valeurs propres semblent se stabiliser à partir de la composante 10. Nous pourrions, donc, garder que les 9 premières composantes. Pour être conservateurs, nous conservons, donc, les 9 premières composantes.

Nous allons afficher les trois premières composantes avec la variable de débit (qui n'a pas été inclut dans cette analyse des composantes principales). Celles-ci conservent à elles seules plus de 50% de la variabilité du jeu de données. Nous affichons que les `(nb_var <- 10)` variables qui contribuent le plus à la variabilité de ces composantes.

<div style="display: flex; justify-content: center;">

<div style="padding-right: 80px;">

**Composantes #1 et #2**

```{r, fig.height=6, fig.width=6, fig.align="c"}


# Afficher la composante #1 et #2
graph_pca(pca, nb_var = nb_var)


```

</div>

<div style="padding-right: 80px;">

**Composantes #2 et #3**

```{r, fig.height=6, fig.width=6, fig.align="c"}


# Afficher la composante #2 et #3
graph_pca(pca, components = c(2, 3), nb_var = nb_var)


```

</div>

</div>

- Modèle _Lasso_

# Analyse, discussion et conclusion (2-3 pages)

- Performance des modèles
- Enjeux éthiques
- Recommandation du modèle? 

# Annexe

